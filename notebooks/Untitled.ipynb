{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54881e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "\n",
    "# Often instead of just checking if two faces match or not (True or False), it's helpful to see how similar they are.\n",
    "# You can do that by using the face_distance function.\n",
    "\n",
    "# The model was trained in a way that faces with a distance of 0.6 or less should be a match. But if you want to\n",
    "# be more strict, you can look for a smaller face distance. For example, using a 0.55 cutoff would reduce false\n",
    "# positive matches at the risk of more false negatives.\n",
    "\n",
    "# Note: This isn't exactly the same as a \"percent match\". The scale isn't linear. But you can assume that images with a\n",
    "# smaller distance are more similar to each other than ones with a larger distance.\n",
    "\n",
    "# Load some images to compare against\n",
    "image1= face_recognition.load_image_file(\"../raw_data/1.jpg\")\n",
    "image2= face_recognition.load_image_file(\"../raw_data/2.jpg\")\n",
    "image3= face_recognition.load_image_file(\"../raw_data/3.jpg\")\n",
    "#image4= face_recognition.load_image_file(\"../raw_data/4.jpg\")\n",
    "#image5= face_recognition.load_image_file(\"../raw_data/5.jpg\")\n",
    "image6= face_recognition.load_image_file(\"../raw_data/6.jpg\")\n",
    "image7= face_recognition.load_image_file(\"../raw_data/7.jpg\")\n",
    "image8= face_recognition.load_image_file(\"../raw_data/8.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e042e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding1 = face_recognition.face_encodings(image1)[0]\n",
    "encoding2 = face_recognition.face_encodings(image2)[0]\n",
    "encoding3 = face_recognition.face_encodings(image3)[0]\n",
    "#encoding4 = face_recognition.face_encodings(image4)[0]\n",
    "#encoding5 = face_recognition.face_encodings(image5)[0]\n",
    "encoding6 = face_recognition.face_encodings(image6)[0]\n",
    "encoding7 = face_recognition.face_encodings(image7)[0]\n",
    "encoding8 = face_recognition.face_encodings(image8)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ca31696",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = (encoding1+encoding2+encoding3+encoding6+encoding7+encoding8) / 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "140d110a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05895312,  0.0745253 ,  0.03380865, -0.04731128, -0.08491885,\n",
       "       -0.02961966, -0.05191092, -0.10862959,  0.14372534, -0.07666109,\n",
       "        0.27531608, -0.00215914, -0.22524711, -0.10346345, -0.02881772,\n",
       "        0.10590863, -0.15179179, -0.10992581, -0.01464245, -0.03897626,\n",
       "        0.09400089,  0.03049925,  0.04361126,  0.02129004, -0.11924487,\n",
       "       -0.30149831, -0.06317274, -0.0645417 ,  0.06719439, -0.10945869,\n",
       "        0.0076423 ,  0.05229391, -0.19245138, -0.06429987,  0.02065386,\n",
       "        0.08652538, -0.07043852, -0.06225016,  0.19108021,  0.03459134,\n",
       "       -0.14663414,  0.01323044, -0.02724313,  0.27925713,  0.18702968,\n",
       "        0.03974114,  0.02003187, -0.15192135,  0.10985727, -0.24433079,\n",
       "        0.07107684,  0.16333052,  0.07042256,  0.06401812,  0.0436044 ,\n",
       "       -0.15644974,  0.04818465,  0.10099875, -0.19695896,  0.06098775,\n",
       "        0.06687803, -0.02233723,  0.00497208, -0.08505531,  0.21065185,\n",
       "        0.05436892, -0.11417659, -0.14432085,  0.11005539, -0.13054259,\n",
       "       -0.10125365,  0.11344694, -0.13645897, -0.19573336, -0.29942531,\n",
       "        0.02756927,  0.35478593,  0.12477077, -0.18834226,  0.00852417,\n",
       "       -0.05529355, -0.00863172,  0.10900245,  0.06525503, -0.03068945,\n",
       "       -0.02844287, -0.10301634,  0.00854152,  0.19495285, -0.03673798,\n",
       "       -0.09611909,  0.23435274, -0.00139264,  0.06649028,  0.05265563,\n",
       "        0.05891484, -0.07191548,  0.03963904, -0.13435482, -0.03143024,\n",
       "        0.04344594, -0.07645156, -0.00183903,  0.13006889, -0.15507843,\n",
       "        0.17883338, -0.01178621,  0.01827412,  0.01456133, -0.05132867,\n",
       "       -0.13738021, -0.00994921,  0.15100468, -0.28005914,  0.26580951,\n",
       "        0.18423581,  0.10626658,  0.12092329,  0.08839736,  0.04223279,\n",
       "       -0.02754693, -0.0313828 , -0.16761477, -0.04406595,  0.02360056,\n",
       "       -0.02829835,  0.05815861,  0.00882281])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bf8b3d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_to_test_encoding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-56f7add01b65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mface_recognition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mface_distance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_to_test_encoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'image_to_test_encoding' is not defined"
     ]
    }
   ],
   "source": [
    "face_recognition.face_distance([avg], image_to_test_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fae1f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test image has a distance of 0.63 from known image #0\n",
      "- With a normal cutoff of 0.6, would the test image match the known image? False\n",
      "- With a very strict cutoff of 0.5, would the test image match the known image? False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "known_encodings = [\n",
    "    encoding1,\n",
    "    encoding2,\n",
    "    encoding3,\n",
    "    encoding6,\n",
    "    encoding7,\n",
    "    encoding8\n",
    "]\n",
    "\n",
    "# Load a test image and get encondings for it\n",
    "test_image=\"../raw_data/no/2.jpg\"\n",
    "image_to_test = face_recognition.load_image_file(test_image)\n",
    "image_to_test_encoding = face_recognition.face_encodings(image_to_test)[0]\n",
    "\n",
    "# See how far apart the test image is from the known faces\n",
    "face_distances = face_recognition.face_distance([avg], image_to_test_encoding)\n",
    "\n",
    "for i, face_distance in enumerate(face_distances):\n",
    "    print(\"The test image has a distance of {:.2} from known image #{}\".format(face_distance, i))\n",
    "    print(\"- With a normal cutoff of 0.6, would the test image match the known image? {}\".format(face_distance < 0.6))\n",
    "    print(\"- With a very strict cutoff of 0.5, would the test image match the known image? {}\".format(face_distance < 0.5))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adf729b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test image has a distance of 0.66 from known image #0\n",
      "- With a normal cutoff of 0.6, would the test image match the known image? False\n",
      "- With a very strict cutoff of 0.5, would the test image match the known image? False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_image=\"../raw_data/no/1.jpg\"\n",
    "image_to_test = face_recognition.load_image_file(test_image)\n",
    "image_to_test_encoding = face_recognition.face_encodings(image_to_test)[0]\n",
    "\n",
    "# See how far apart the test image is from the known faces\n",
    "face_distances = face_recognition.face_distance([avg], image_to_test_encoding)\n",
    "\n",
    "for i, face_distance in enumerate(face_distances):\n",
    "    print(\"The test image has a distance of {:.2} from known image #{}\".format(face_distance, i))\n",
    "    print(\"- With a normal cutoff of 0.6, would the test image match the known image? {}\".format(face_distance < 0.6))\n",
    "    print(\"- With a very strict cutoff of 0.5, would the test image match the known image? {}\".format(face_distance < 0.5))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da246b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61ab3d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(image):\n",
    "    avg=np.array([-0.05895312,  0.0745253 ,  0.03380865, -0.04731128, -0.08491885,\n",
    "       -0.02961966, -0.05191092, -0.10862959,  0.14372534, -0.07666109,\n",
    "        0.27531608, -0.00215914, -0.22524711, -0.10346345, -0.02881772,\n",
    "        0.10590863, -0.15179179, -0.10992581, -0.01464245, -0.03897626,\n",
    "        0.09400089,  0.03049925,  0.04361126,  0.02129004, -0.11924487,\n",
    "       -0.30149831, -0.06317274, -0.0645417 ,  0.06719439, -0.10945869,\n",
    "        0.0076423 ,  0.05229391, -0.19245138, -0.06429987,  0.02065386,\n",
    "        0.08652538, -0.07043852, -0.06225016,  0.19108021,  0.03459134,\n",
    "       -0.14663414,  0.01323044, -0.02724313,  0.27925713,  0.18702968,\n",
    "        0.03974114,  0.02003187, -0.15192135,  0.10985727, -0.24433079,\n",
    "        0.07107684,  0.16333052,  0.07042256,  0.06401812,  0.0436044 ,\n",
    "       -0.15644974,  0.04818465,  0.10099875, -0.19695896,  0.06098775,\n",
    "        0.06687803, -0.02233723,  0.00497208, -0.08505531,  0.21065185,\n",
    "        0.05436892, -0.11417659, -0.14432085,  0.11005539, -0.13054259,\n",
    "       -0.10125365,  0.11344694, -0.13645897, -0.19573336, -0.29942531,\n",
    "        0.02756927,  0.35478593,  0.12477077, -0.18834226,  0.00852417,\n",
    "       -0.05529355, -0.00863172,  0.10900245,  0.06525503, -0.03068945,\n",
    "       -0.02844287, -0.10301634,  0.00854152,  0.19495285, -0.03673798,\n",
    "       -0.09611909,  0.23435274, -0.00139264,  0.06649028,  0.05265563,\n",
    "        0.05891484, -0.07191548,  0.03963904, -0.13435482, -0.03143024,\n",
    "        0.04344594, -0.07645156, -0.00183903,  0.13006889, -0.15507843,\n",
    "        0.17883338, -0.01178621,  0.01827412,  0.01456133, -0.05132867,\n",
    "       -0.13738021, -0.00994921,  0.15100468, -0.28005914,  0.26580951,\n",
    "        0.18423581,  0.10626658,  0.12092329,  0.08839736,  0.04223279,\n",
    "       -0.02754693, -0.0313828 , -0.16761477, -0.04406595,  0.02360056,\n",
    "       -0.02829835,  0.05815861,  0.00882281])\n",
    "    image_to_test = face_recognition.load_image_file(image)\n",
    "    image_to_test_encoding = face_recognition.face_encodings(image_to_test)[0]\n",
    "    face_distances = face_recognition.face_distance([avg], image_to_test_encoding)\n",
    "    if face_distances[0]<0.70:\n",
    "        return 'Match!'\n",
    "    else:\n",
    "        return 'not a match'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2ecd287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Match!'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare('/Users/shan/Desktop/1091627873644_.pic_hd.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "c8d9f625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.12424845,  0.03146885,  0.05647903, -0.10183209, -0.08059764,\n",
       "         0.00125573, -0.08470078, -0.20894933,  0.13960624, -0.15204534,\n",
       "         0.18837392, -0.08571739, -0.10731141, -0.06414987, -0.07164326,\n",
       "         0.22563726, -0.20380116, -0.15313897, -0.04225398, -0.00913167,\n",
       "         0.04848053,  0.06787235,  0.01254821,  0.04795593, -0.15805431,\n",
       "        -0.33931401, -0.12057308, -0.08190247, -0.02016748, -0.02055447,\n",
       "         0.00418332,  0.06671431, -0.17262483,  0.01699103,  0.05843588,\n",
       "         0.13852718,  0.01816491, -0.07862682,  0.14714946, -0.01128905,\n",
       "        -0.27818319,  0.08660196,  0.1316317 ,  0.2293763 ,  0.19031867,\n",
       "        -0.04441412, -0.02133941, -0.13602546,  0.14833185, -0.15973698,\n",
       "         0.00672863,  0.072104  ,  0.05713584,  0.04492862,  0.07480467,\n",
       "        -0.0520949 ,  0.04767701,  0.13474607, -0.12153506,  0.02380858,\n",
       "         0.10781275, -0.08982941,  0.01063498, -0.08109361,  0.16271685,\n",
       "         0.1025099 , -0.09154227, -0.20728832,  0.09373081, -0.12851009,\n",
       "        -0.09558642,  0.0585442 , -0.1395959 , -0.16199464, -0.29667401,\n",
       "        -0.0360418 ,  0.37501529,  0.10420252, -0.22063786,  0.0842802 ,\n",
       "         0.02275172,  0.01027026,  0.16614634,  0.17836255,  0.05735096,\n",
       "         0.0192669 , -0.05374135,  0.00350593,  0.26313427, -0.04416228,\n",
       "         0.01174395,  0.25243193, -0.03144399,  0.0453873 ,  0.01066985,\n",
       "         0.01028366, -0.08534308,  0.05363147, -0.13458231, -0.01797955,\n",
       "         0.00534378,  0.06362966,  0.03114457,  0.11682591, -0.14393966,\n",
       "         0.16422981, -0.01109725,  0.03589016,  0.05020116,  0.02717625,\n",
       "        -0.0391143 , -0.03367795,  0.12965931, -0.19928689,  0.16221127,\n",
       "         0.13332355,  0.02255538,  0.09255745,  0.10706455,  0.08103609,\n",
       "        -0.01929389,  0.04281864, -0.20923783,  0.0055982 ,  0.09022872,\n",
       "        -0.08113568,  0.06419751,  0.00670002]),\n",
       " array([-0.06573559,  0.09549704, -0.00918434, -0.11786555, -0.18093045,\n",
       "         0.03810094,  0.00830932, -0.10858498,  0.14384329, -0.03838942,\n",
       "         0.19070433,  0.06182015, -0.24625772, -0.08214608,  0.00975077,\n",
       "         0.09167249, -0.14704669, -0.16105658, -0.0544166 , -0.17263162,\n",
       "        -0.05782938,  0.00057102, -0.02205215,  0.01228986, -0.15115753,\n",
       "        -0.21082982, -0.12467651, -0.04830696,  0.20177448, -0.11730924,\n",
       "         0.01846329,  0.042622  , -0.18485203, -0.02755167,  0.03659932,\n",
       "         0.07217327, -0.07749531, -0.11209048,  0.16869408,  0.03388683,\n",
       "        -0.09734447, -0.01508491,  0.06600414,  0.25362456,  0.15394834,\n",
       "         0.07725826,  0.07933169, -0.06733473,  0.12702884, -0.25690043,\n",
       "         0.1293339 ,  0.09190857,  0.07504952,  0.03253216,  0.04135606,\n",
       "        -0.21412559, -0.01152667,  0.12386494, -0.1982331 ,  0.09347728,\n",
       "         0.05333015, -0.13491328, -0.1360383 , -0.0646123 ,  0.22914693,\n",
       "         0.16480404, -0.05919199, -0.18433285,  0.32825103, -0.17115788,\n",
       "        -0.12213519,  0.04887909, -0.03237675, -0.04753658, -0.28583121,\n",
       "         0.10367343,  0.4108918 ,  0.15926442, -0.07745898,  0.04421218,\n",
       "        -0.02886853,  0.00656169,  0.00871   ,  0.05758862, -0.17452236,\n",
       "        -0.07260922, -0.10974181,  0.04295866,  0.24224353, -0.03193681,\n",
       "        -0.00866397,  0.15348652,  0.07365753,  0.07759192,  0.02871106,\n",
       "         0.05909765, -0.10095415, -0.01897166, -0.08603089, -0.01717801,\n",
       "         0.12380219, -0.0875545 ,  0.04971214,  0.1928892 , -0.16038616,\n",
       "         0.18723682,  0.00445767, -0.0958631 , -0.0292314 , -0.03316855,\n",
       "        -0.02472083, -0.00626968,  0.13017035, -0.30064985,  0.23966706,\n",
       "         0.19375846,  0.05952813,  0.12931021,  0.07781103,  0.05470549,\n",
       "         0.06498685, -0.09156112, -0.15453126, -0.13628921,  0.00493496,\n",
       "        -0.10960669, -0.02228405,  0.11007424]),\n",
       " array([-0.09730637,  0.03725155,  0.12879053, -0.08467378, -0.09758709,\n",
       "         0.01015475, -0.07577556, -0.03111972,  0.12428027, -0.2216126 ,\n",
       "         0.15304863, -0.11352002, -0.20060416,  0.04130764, -0.01590185,\n",
       "         0.19558311, -0.17215948, -0.14838356, -0.02652006,  0.0074138 ,\n",
       "         0.06498123,  0.03750178, -0.02210811, -0.0103156 , -0.12624699,\n",
       "        -0.38855827, -0.10990693,  0.02729156, -0.00900476, -0.02652991,\n",
       "        -0.08919856,  0.01515893, -0.18652806,  0.00683574, -0.00777852,\n",
       "         0.11670847, -0.00819675, -0.11782934,  0.14594084,  0.01659385,\n",
       "        -0.29119563,  0.05406635,  0.06437379,  0.22721039,  0.19721064,\n",
       "         0.00558052, -0.0247407 , -0.07291935,  0.11256247, -0.21935827,\n",
       "         0.01302121,  0.1131333 ,  0.04940534,  0.03811748,  0.04545832,\n",
       "        -0.0599505 ,  0.00483361,  0.16177787, -0.10670455, -0.01012109,\n",
       "         0.09896237, -0.06816774,  0.04781654, -0.15484005,  0.20082653,\n",
       "         0.0657784 , -0.10552529, -0.20182684,  0.14004381, -0.14445788,\n",
       "        -0.05667777,  0.03236317, -0.1155744 , -0.15275265, -0.27157152,\n",
       "         0.01692647,  0.30010644,  0.12470414, -0.13223456,  0.09567757,\n",
       "         0.0104183 , -0.01537606,  0.12413861,  0.18577349,  0.03966507,\n",
       "         0.05618848, -0.04036887,  0.01029219,  0.25995997, -0.03465755,\n",
       "         0.00173477,  0.2041712 , -0.01713923,  0.04018824,  0.02684179,\n",
       "        -0.00248689, -0.09314176,  0.02761823, -0.16412175,  0.03780602,\n",
       "        -0.02447238, -0.04852861, -0.03430733,  0.15760206, -0.15429987,\n",
       "         0.14484791, -0.06147891,  0.01843397,  0.03655542,  0.02904993,\n",
       "        -0.07323641, -0.0574715 ,  0.09832597, -0.24221721,  0.06532847,\n",
       "         0.18041983,  0.02896804,  0.09854294,  0.11772545,  0.08983019,\n",
       "         0.01522733,  0.05483128, -0.27545875, -0.03105802,  0.07788154,\n",
       "        -0.04938464,  0.06135347,  0.04779485]),\n",
       " array([-5.10654375e-02,  8.60984474e-02,  1.49355996e-02, -8.08083266e-02,\n",
       "        -1.27385154e-01, -2.57966146e-02, -8.78300294e-02, -4.36622538e-02,\n",
       "         1.61794096e-01, -9.64824110e-02,  1.75299034e-01, -7.87327364e-02,\n",
       "        -1.98865861e-01, -3.84680331e-02, -9.26359817e-02,  2.35285416e-01,\n",
       "        -2.01079175e-01, -1.59096524e-01, -5.16024828e-02,  4.03854698e-02,\n",
       "         4.69987057e-02,  9.27029699e-02,  7.84768611e-02,  5.10034934e-02,\n",
       "        -1.44355088e-01, -3.50218475e-01, -9.35545787e-02, -7.58851245e-02,\n",
       "        -9.39071774e-02, -7.03860968e-02, -2.95646563e-02,  1.12570405e-01,\n",
       "        -1.76277772e-01, -1.77711248e-02,  8.55125785e-02,  1.07639447e-01,\n",
       "        -3.99202108e-05, -7.47982711e-02,  1.80540845e-01,  3.68341245e-03,\n",
       "        -2.95189798e-01,  8.62576663e-02,  1.11418545e-01,  3.09627354e-01,\n",
       "         2.04241335e-01, -2.64153220e-02, -1.56596936e-02, -1.32713705e-01,\n",
       "         1.24648489e-01, -1.60836235e-01, -1.89491063e-02,  1.40811697e-01,\n",
       "         5.32411672e-02,  2.09852234e-02,  2.08221152e-02, -5.04127480e-02,\n",
       "         7.54665136e-02,  1.43362373e-01, -1.06633492e-01, -6.58208579e-02,\n",
       "         1.02797046e-01, -7.36774728e-02,  2.52837688e-03, -1.09878875e-01,\n",
       "         2.25092188e-01,  6.48284052e-03, -1.15832962e-01, -2.16058999e-01,\n",
       "         7.79839382e-02, -1.40784681e-01, -8.88334140e-02,  9.22465995e-02,\n",
       "        -1.74320400e-01, -1.73238978e-01, -3.13655734e-01, -5.20336479e-02,\n",
       "         2.87707508e-01,  8.12743977e-02, -1.84429586e-01,  1.48666188e-01,\n",
       "        -3.67166079e-03,  4.43488732e-02,  6.71465471e-02,  1.86497793e-01,\n",
       "        -1.13459565e-02,  9.96554717e-02, -1.38725072e-01,  2.45864093e-02,\n",
       "         2.71655679e-01, -1.17310375e-01,  2.74328981e-03,  2.29912579e-01,\n",
       "         2.34803259e-02,  3.82099673e-02,  5.12683764e-02, -2.23007575e-02,\n",
       "        -6.09841719e-02,  1.82345361e-02, -2.05276459e-01,  3.02844793e-02,\n",
       "         1.33769661e-02,  1.18232518e-03, -6.08219504e-02,  1.08454309e-01,\n",
       "        -1.48957610e-01,  7.39600509e-02, -2.02274472e-02,  2.78625265e-03,\n",
       "        -1.23393014e-02, -8.98633897e-03, -7.98740089e-02, -9.16477889e-02,\n",
       "         1.29222617e-01, -2.21317768e-01,  1.30863696e-01,  1.29480094e-01,\n",
       "         6.64845705e-02,  8.57777223e-02,  1.41618058e-01,  7.72787258e-02,\n",
       "        -6.73663840e-02,  2.03404725e-02, -2.10870609e-01, -8.97367857e-03,\n",
       "         1.45517275e-01, -2.30904371e-02,  1.05935201e-01, -3.91867757e-03]),\n",
       " array([-1.38458967e-01,  4.12710235e-02, -2.71598548e-02, -2.80576833e-02,\n",
       "        -8.39381367e-02, -9.95874479e-02, -3.11879143e-02, -1.40340254e-01,\n",
       "         1.35302633e-01, -9.00326371e-02,  1.81601718e-01, -1.55928507e-02,\n",
       "        -1.81885839e-01, -1.23213708e-01, -7.62800425e-02,  1.73151374e-01,\n",
       "        -2.14678466e-01, -1.46642700e-01, -8.17003846e-03,  2.14982480e-02,\n",
       "         1.49848595e-01,  3.94814927e-03,  2.74527818e-04,  2.28461251e-02,\n",
       "        -1.06981188e-01, -2.97308385e-01, -6.47958815e-02, -3.10793854e-02,\n",
       "         4.42426652e-03, -1.63846463e-02,  1.17598847e-02,  6.85802698e-02,\n",
       "        -2.03593552e-01, -6.25806600e-02,  6.67740256e-02,  1.07359506e-01,\n",
       "         1.81885809e-03, -1.87263712e-02,  2.19082847e-01,  5.97591698e-02,\n",
       "        -2.47478202e-01, -1.52094066e-02,  8.02042186e-02,  2.85716295e-01,\n",
       "         2.36076221e-01,  3.00233085e-02, -4.67781536e-03, -1.24070480e-01,\n",
       "         1.78728357e-01, -1.80688679e-01, -1.00002531e-02,  1.52599946e-01,\n",
       "         1.26842275e-01,  2.76424363e-02,  1.31249428e-04, -1.13458306e-01,\n",
       "         3.48978341e-02,  9.51285809e-02, -1.59496307e-01,  2.25940887e-02,\n",
       "         1.08494781e-01, -9.10601467e-02, -5.41332513e-02, -8.15320611e-02,\n",
       "         1.94258615e-01,  8.41153134e-03, -1.48592174e-01, -1.74649671e-01,\n",
       "         1.24773249e-01, -1.56286240e-01, -1.27148688e-01,  4.94498387e-02,\n",
       "        -1.40802234e-01, -1.82091236e-01, -3.26943099e-01, -4.94334996e-02,\n",
       "         3.87796104e-01,  6.77178502e-02, -1.78750664e-01,  6.53247982e-02,\n",
       "        -1.11436378e-02, -1.18142068e-02,  1.37725890e-01,  1.75455198e-01,\n",
       "         7.17249513e-03,  5.35530522e-02, -8.20282549e-02, -3.89938802e-02,\n",
       "         2.33637050e-01, -3.10071036e-02, -6.45470992e-02,  2.01807603e-01,\n",
       "        -1.57058463e-02,  6.72442243e-02,  2.22395211e-02,  1.60783157e-02,\n",
       "        -4.73061576e-02,  4.90886346e-02, -1.34015217e-01, -5.40777519e-02,\n",
       "         4.36977372e-02, -7.07871541e-02,  3.66548598e-02,  1.12210929e-01,\n",
       "        -1.37815237e-01,  1.39220506e-01, -7.22489506e-03,  1.07240558e-01,\n",
       "         1.66518390e-02, -3.71220112e-02, -1.41289517e-01, -2.18134671e-02,\n",
       "         8.70600492e-02, -1.89662665e-01,  2.42606863e-01,  1.61355108e-01,\n",
       "         4.13794443e-02,  7.85081685e-02,  1.38537347e-01,  6.01268858e-02,\n",
       "        -4.71222401e-03,  4.48778570e-02, -1.85029760e-01, -5.29978722e-02,\n",
       "         4.59490493e-02, -4.04116958e-02,  1.41882509e-01, -2.05552392e-02]),\n",
       " array([-7.69175217e-02,  1.38649955e-01,  8.53071064e-02,  6.30234405e-02,\n",
       "        -1.18166283e-01,  3.85761522e-02, -4.81094010e-02, -1.15511268e-01,\n",
       "         1.02626316e-01, -6.43572137e-02,  2.79670507e-01,  7.11324811e-03,\n",
       "        -1.92988306e-01, -6.08974099e-02, -1.62706077e-02,  1.42370582e-01,\n",
       "        -1.99599817e-01, -1.19601198e-01, -8.85040164e-02, -7.27503374e-02,\n",
       "         7.58043677e-02,  4.61797491e-02,  5.85522428e-02,  2.52024829e-03,\n",
       "        -1.19952679e-01, -4.13864672e-01, -8.53112787e-02, -6.63344860e-02,\n",
       "         1.43590316e-01, -7.14987516e-02,  1.51937827e-04,  3.99821252e-02,\n",
       "        -1.65013596e-01, -1.77556276e-03,  5.38686216e-02,  5.84661886e-02,\n",
       "        -7.85414428e-02, -3.00829113e-02,  1.78270817e-01, -1.23673221e-02,\n",
       "        -1.52548030e-01,  1.48279086e-01, -2.01691166e-02,  3.24935973e-01,\n",
       "         1.65321052e-01,  5.84733263e-02,  3.68436053e-02, -1.20343894e-01,\n",
       "         4.25300151e-02, -2.03761026e-01,  8.76480341e-02,  2.40996525e-01,\n",
       "         1.15978956e-01,  1.09290749e-01,  3.37816775e-02, -6.53031170e-02,\n",
       "        -1.17701292e-02,  1.23117410e-01, -2.06790969e-01,  6.10035136e-02,\n",
       "         7.33646601e-02, -6.31285906e-02,  3.43412198e-02, -1.18650988e-01,\n",
       "         2.03856260e-01,  9.83313620e-02, -1.58706933e-01, -1.06477290e-01,\n",
       "         5.02651595e-02, -1.29244894e-01, -6.77606538e-02,  1.93719119e-02,\n",
       "        -1.40991569e-01, -1.91994578e-01, -2.51950771e-01,  4.39253151e-02,\n",
       "         4.75113988e-01,  1.43536374e-01, -2.43974209e-01, -5.08535095e-03,\n",
       "        -8.91467109e-02, -5.10237217e-02,  7.27113634e-02,  1.34647682e-01,\n",
       "        -4.44551259e-02, -7.75843188e-02, -6.65238425e-02,  8.06016177e-02,\n",
       "         1.57539546e-01, -5.68542853e-02, -4.59437966e-02,  2.59950876e-01,\n",
       "         3.73961814e-02,  3.22256163e-02,  2.92448513e-03,  5.29797375e-02,\n",
       "        -9.53569040e-02, -1.58881694e-02, -8.15373585e-02, -5.17472848e-02,\n",
       "        -9.49149206e-02, -3.11998650e-02, -3.84390354e-03,  4.14844528e-02,\n",
       "        -1.37768999e-01,  2.66033262e-01,  3.95090356e-02,  3.51671875e-02,\n",
       "         4.07403111e-02,  1.10701621e-02, -6.88890293e-02, -3.25548351e-02,\n",
       "         1.33445889e-01, -3.22909623e-01,  2.68282026e-01,  2.05858469e-01,\n",
       "         5.74571118e-02,  1.79435104e-01,  3.48771736e-02,  8.42039511e-02,\n",
       "        -3.15549001e-02, -3.67624313e-02, -1.35771319e-01, -6.38379157e-02,\n",
       "         3.26437019e-02, -4.76678275e-02,  3.54053676e-02,  1.23021677e-02]),\n",
       " array([-0.0423921 ,  0.11541635,  0.03332707, -0.07753636, -0.10883348,\n",
       "        -0.0358927 , -0.07832254, -0.13644183,  0.14654639, -0.12435646,\n",
       "         0.22132704, -0.11392028, -0.18947074, -0.09880928, -0.04671042,\n",
       "         0.14036575, -0.20043878, -0.08614873, -0.01117513,  0.00082284,\n",
       "         0.13130581,  0.05854142,  0.01538073,  0.03760908, -0.09721197,\n",
       "        -0.33632171, -0.1225448 , -0.06181726, -0.0026707 , -0.08963123,\n",
       "        -0.02958042,  0.05879831, -0.23045322, -0.11870186,  0.03369354,\n",
       "         0.07318076, -0.00068905, -0.02237576,  0.17621897,  0.00847253,\n",
       "        -0.24233441,  0.0818602 ,  0.03182217,  0.23699594,  0.16337925,\n",
       "         0.07739261, -0.03302716, -0.16354904,  0.16297722, -0.13733135,\n",
       "         0.06449807,  0.16554579,  0.06203331,  0.0296673 , -0.03679052,\n",
       "        -0.09064856,  0.01669049,  0.12047082, -0.15281701,  0.04201241,\n",
       "         0.15112196, -0.14891095, -0.00885096, -0.04635766,  0.14907588,\n",
       "         0.03075985, -0.08966338, -0.22884972,  0.07935207, -0.14652939,\n",
       "        -0.12034555,  0.09136471, -0.13130522, -0.16039285, -0.3576993 ,\n",
       "         0.01894666,  0.37194157,  0.05949132, -0.1980634 ,  0.03133523,\n",
       "        -0.04239709, -0.00215875,  0.16788737,  0.19035305,  0.03568125,\n",
       "         0.02918817, -0.08103196,  0.03506812,  0.24118042, -0.08422477,\n",
       "        -0.07129788,  0.27379939, -0.02189619,  0.13828583,  0.01608689,\n",
       "         0.04204983, -0.06380997,  0.03876681, -0.13704643,  0.02256036,\n",
       "         0.08465396,  0.00189911, -0.05461076,  0.14701632, -0.16458817,\n",
       "         0.14799774, -0.02403457,  0.02404715, -0.00297537, -0.08076832,\n",
       "        -0.10857423, -0.03606251,  0.15284297, -0.22459249,  0.21705174,\n",
       "         0.16685155,  0.06861841,  0.10463305,  0.15260229,  0.0362888 ,\n",
       "         0.03340293, -0.06201209, -0.25559264, -0.05130819,  0.13783371,\n",
       "        -0.00195204,  0.1032142 , -0.02945706]),\n",
       " array([-0.07878195,  0.08551986,  0.05883117, -0.07861922, -0.1522447 ,\n",
       "         0.00920349,  0.01602934, -0.08125393,  0.13600731,  0.0051068 ,\n",
       "         0.20673791,  0.03211326, -0.24104914, -0.05148976,  0.01908001,\n",
       "         0.12275711, -0.12812638, -0.15232919, -0.13415402, -0.09315142,\n",
       "         0.040953  ,  0.00909183, -0.0381816 ,  0.07464433, -0.17975746,\n",
       "        -0.24941434, -0.07167126, -0.12845355,  0.12255226, -0.18634614,\n",
       "        -0.01122   ,  0.02779259, -0.17159632, -0.047032  ,  0.03428834,\n",
       "         0.0831709 , -0.02815434, -0.14718895,  0.19836961, -0.02670491,\n",
       "        -0.19209027,  0.01508211,  0.05516985,  0.22040486,  0.21992137,\n",
       "         0.01807384,  0.06267334, -0.11236469,  0.14117146, -0.2207651 ,\n",
       "         0.13341966,  0.08527042,  0.08354045,  0.06846859,  0.02668908,\n",
       "        -0.24391553,  0.02684787,  0.14208812, -0.27894789,  0.11878692,\n",
       "         0.05996845, -0.15596247, -0.13231552, -0.10023904,  0.1521699 ,\n",
       "         0.10253681, -0.1085674 , -0.20322341,  0.18859541, -0.2082645 ,\n",
       "        -0.09456089,  0.09311663, -0.12544492, -0.11641206, -0.37814704,\n",
       "         0.04207081,  0.43874872,  0.1473362 , -0.20609888,  0.02004822,\n",
       "        -0.02698392, -0.02031018,  0.00863264,  0.08507451, -0.1338083 ,\n",
       "        -0.02983257, -0.09327768,  0.07171635,  0.2265605 ,  0.04259042,\n",
       "        -0.02144056,  0.21879613,  0.0090323 ,  0.02023798,  0.01989289,\n",
       "         0.08155984, -0.08314827, -0.04461787, -0.05307567, -0.03655279,\n",
       "         0.10282337, -0.12338845,  0.02032432,  0.16960962, -0.24069521,\n",
       "         0.18878838, -0.02470486, -0.00246696, -0.00431807, -0.06383277,\n",
       "        -0.11371322,  0.02161199,  0.25224686, -0.26744848,  0.20987841,\n",
       "         0.23268637,  0.11268187,  0.07156447,  0.08387123,  0.04124153,\n",
       "         0.00280984, -0.04374191, -0.26387078, -0.01548077,  0.07409655,\n",
       "        -0.13126223,  0.08348209,  0.0932033 ])]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image=('/Users/shan/Desktop/1091627873644_.pic_hd.jpg')\n",
    "image_to_test = face_recognition.load_image_file(image)\n",
    "image_to_test_encoding = face_recognition.face_encodings(image_to_test)\n",
    "image_to_test_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "a696908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "\n",
    "for encode in image_to_test_encoding:\n",
    "    face_distances = face_recognition.face_distance([avg], encode)\n",
    "    if face_distances[0]<0.60:\n",
    "        results.append( 'Match!')\n",
    "    else:\n",
    "        results.append( 'forever alone')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "2dd07000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found 8 face(s) in this photograph.\n",
      "The chin in this face has the following points: [(562, 783), (562, 786), (562, 790), (563, 794), (565, 797), (567, 800), (569, 804), (571, 807), (574, 808), (578, 807), (582, 805), (586, 802), (589, 799), (592, 794), (593, 790), (594, 785), (593, 780)]\n",
      "The left_eyebrow in this face has the following points: [(562, 776), (563, 775), (565, 775), (566, 775), (567, 776)]\n",
      "The right_eyebrow in this face has the following points: [(573, 775), (575, 774), (579, 773), (582, 773), (584, 775)]\n",
      "The nose_bridge in this face has the following points: [(570, 781), (570, 784), (569, 786), (569, 789)]\n",
      "The nose_tip in this face has the following points: [(568, 791), (569, 792), (570, 792), (572, 792), (573, 791)]\n",
      "The left_eye in this face has the following points: [(564, 782), (565, 781), (566, 781), (568, 782), (566, 783), (565, 783)]\n",
      "The right_eye in this face has the following points: [(575, 781), (576, 779), (579, 779), (581, 780), (579, 781), (577, 782)]\n",
      "The top_lip in this face has the following points: [(569, 797), (568, 796), (570, 795), (571, 795), (572, 794), (575, 795), (578, 797), (577, 797), (573, 796), (571, 797), (570, 797), (569, 797)]\n",
      "The bottom_lip in this face has the following points: [(578, 797), (576, 799), (574, 801), (572, 801), (571, 801), (569, 799), (569, 797), (569, 797), (570, 798), (571, 798), (573, 798), (577, 797)]\n",
      "The chin in this face has the following points: [(814, 522), (816, 535), (819, 548), (822, 561), (827, 574), (834, 585), (844, 594), (855, 601), (867, 602), (881, 599), (895, 591), (909, 581), (920, 568), (926, 553), (927, 537), (926, 521), (924, 504)]\n",
      "The left_eyebrow in this face has the following points: [(817, 516), (821, 510), (829, 507), (837, 507), (845, 510)]\n",
      "The right_eyebrow in this face has the following points: [(864, 508), (873, 503), (882, 501), (891, 501), (900, 504)]\n",
      "The nose_bridge in this face has the following points: [(855, 519), (856, 529), (856, 539), (856, 549)]\n",
      "The nose_tip in this face has the following points: [(849, 553), (853, 555), (859, 557), (865, 553), (871, 550)]\n",
      "The left_eye in this face has the following points: [(827, 524), (833, 520), (839, 519), (846, 522), (840, 524), (833, 525)]\n",
      "The right_eye in this face has the following points: [(872, 519), (877, 515), (884, 514), (890, 515), (885, 518), (878, 519)]\n",
      "The top_lip in this face has the following points: [(843, 566), (850, 564), (857, 563), (862, 563), (868, 561), (877, 560), (888, 560), (885, 562), (868, 564), (863, 565), (857, 565), (846, 566)]\n",
      "The bottom_lip in this face has the following points: [(888, 560), (879, 570), (870, 574), (864, 576), (858, 576), (850, 574), (843, 566), (846, 566), (857, 571), (863, 571), (869, 570), (885, 562)]\n",
      "The chin in this face has the following points: [(1126, 491), (1126, 502), (1128, 512), (1131, 522), (1136, 532), (1142, 540), (1150, 547), (1159, 552), (1168, 554), (1179, 552), (1189, 546), (1198, 539), (1206, 530), (1210, 519), (1212, 507), (1214, 494), (1214, 482)]\n",
      "The left_eyebrow in this face has the following points: [(1129, 478), (1133, 473), (1139, 471), (1146, 471), (1153, 472)]\n",
      "The right_eyebrow in this face has the following points: [(1167, 470), (1175, 466), (1184, 464), (1192, 466), (1199, 470)]\n",
      "The nose_bridge in this face has the following points: [(1160, 483), (1160, 491), (1160, 499), (1160, 507)]\n",
      "The nose_tip in this face has the following points: [(1156, 513), (1159, 514), (1162, 515), (1166, 513), (1170, 512)]\n",
      "The left_eye in this face has the following points: [(1136, 487), (1140, 483), (1146, 483), (1151, 487), (1146, 488), (1141, 488)]\n",
      "The right_eye in this face has the following points: [(1173, 484), (1178, 479), (1184, 478), (1190, 481), (1185, 483), (1179, 484)]\n",
      "The top_lip in this face has the following points: [(1151, 527), (1155, 523), (1160, 521), (1164, 522), (1168, 520), (1175, 521), (1183, 524), (1180, 524), (1169, 525), (1164, 525), (1160, 525), (1153, 527)]\n",
      "The bottom_lip in this face has the following points: [(1183, 524), (1176, 530), (1170, 533), (1165, 533), (1161, 533), (1156, 531), (1151, 527), (1153, 527), (1160, 528), (1164, 528), (1169, 527), (1180, 524)]\n",
      "The chin in this face has the following points: [(499, 136), (498, 150), (500, 165), (503, 179), (509, 192), (517, 204), (528, 214), (538, 222), (550, 224), (563, 223), (575, 216), (586, 207), (595, 196), (601, 183), (605, 168), (607, 154), (607, 139)]\n",
      "The left_eyebrow in this face has the following points: [(503, 120), (511, 113), (523, 110), (534, 111), (544, 116)]\n",
      "The right_eyebrow in this face has the following points: [(558, 116), (568, 113), (580, 112), (590, 114), (598, 121)]\n",
      "The nose_bridge in this face has the following points: [(549, 132), (549, 142), (549, 152), (548, 163)]\n",
      "The nose_tip in this face has the following points: [(538, 172), (544, 173), (549, 174), (554, 173), (559, 172)]\n",
      "The left_eye in this face has the following points: [(511, 133), (518, 129), (526, 129), (533, 136), (526, 137), (517, 137)]\n",
      "The right_eye in this face has the following points: [(566, 136), (573, 130), (581, 130), (588, 134), (582, 137), (574, 138)]\n",
      "The top_lip in this face has the following points: [(529, 190), (538, 188), (544, 185), (548, 187), (553, 186), (560, 188), (570, 190), (566, 191), (553, 190), (548, 191), (544, 190), (533, 190)]\n",
      "The bottom_lip in this face has the following points: [(570, 190), (560, 196), (553, 199), (548, 199), (543, 198), (537, 196), (529, 190), (533, 190), (544, 191), (548, 191), (553, 191), (566, 191)]\n",
      "The chin in this face has the following points: [(217, 831), (216, 837), (215, 843), (215, 849), (215, 855), (217, 860), (222, 865), (227, 868), (234, 869), (242, 870), (250, 868), (256, 866), (260, 862), (263, 857), (264, 851), (266, 846), (267, 841)]\n",
      "The left_eyebrow in this face has the following points: [(224, 824), (228, 822), (232, 822), (236, 823), (239, 825)]\n",
      "The right_eyebrow in this face has the following points: [(248, 827), (252, 826), (256, 826), (260, 828), (262, 831)]\n",
      "The nose_bridge in this face has the following points: [(242, 832), (242, 835), (241, 839), (240, 842)]\n",
      "The nose_tip in this face has the following points: [(235, 846), (237, 847), (239, 848), (242, 848), (244, 847)]\n",
      "The left_eye in this face has the following points: [(227, 831), (230, 830), (233, 830), (235, 833), (232, 833), (229, 832)]\n",
      "The right_eye in this face has the following points: [(249, 835), (252, 834), (255, 834), (257, 836), (255, 837), (252, 836)]\n",
      "The top_lip in this face has the following points: [(228, 852), (232, 851), (236, 851), (238, 851), (241, 851), (244, 853), (248, 855), (246, 855), (241, 853), (238, 853), (235, 852), (229, 852)]\n",
      "The bottom_lip in this face has the following points: [(248, 855), (243, 856), (240, 856), (237, 855), (234, 855), (231, 854), (228, 852), (229, 852), (235, 852), (238, 853), (240, 853), (246, 855)]\n",
      "The chin in this face has the following points: [(171, 163), (170, 183), (172, 203), (175, 223), (182, 240), (191, 256), (206, 266), (223, 272), (243, 275), (264, 275), (282, 270), (299, 260), (311, 246), (320, 229), (326, 208), (328, 188), (329, 167)]\n",
      "The left_eyebrow in this face has the following points: [(183, 133), (193, 123), (206, 119), (222, 119), (235, 126)]\n",
      "The right_eyebrow in this face has the following points: [(256, 125), (271, 120), (288, 121), (302, 126), (312, 138)]\n",
      "The nose_bridge in this face has the following points: [(247, 141), (247, 151), (246, 160), (246, 170)]\n",
      "The nose_tip in this face has the following points: [(229, 188), (237, 190), (246, 192), (254, 190), (263, 189)]\n",
      "The left_eye in this face has the following points: [(199, 145), (207, 140), (217, 139), (226, 146), (217, 148), (207, 148)]\n",
      "The right_eye in this face has the following points: [(268, 147), (278, 141), (288, 142), (297, 148), (288, 151), (278, 150)]\n",
      "The top_lip in this face has the following points: [(216, 224), (225, 211), (237, 206), (244, 208), (253, 207), (264, 213), (273, 227), (267, 225), (252, 216), (244, 216), (237, 215), (220, 222)]\n",
      "The bottom_lip in this face has the following points: [(273, 227), (263, 231), (252, 233), (244, 233), (236, 232), (224, 229), (216, 224), (220, 222), (236, 222), (244, 223), (252, 223), (267, 225)]\n",
      "The chin in this face has the following points: [(523, 494), (525, 506), (528, 518), (533, 528), (540, 537), (549, 544), (560, 550), (571, 554), (581, 553), (588, 550), (593, 543), (597, 534), (599, 525), (601, 516), (602, 507), (602, 498), (601, 489)]\n",
      "The left_eyebrow in this face has the following points: [(539, 484), (546, 479), (554, 477), (562, 477), (570, 480)]\n",
      "The right_eyebrow in this face has the following points: [(583, 479), (588, 476), (593, 474), (598, 474), (601, 478)]\n",
      "The nose_bridge in this face has the following points: [(578, 489), (580, 496), (581, 502), (583, 509)]\n",
      "The nose_tip in this face has the following points: [(574, 516), (578, 517), (582, 517), (584, 516), (587, 514)]\n",
      "The left_eye in this face has the following points: [(551, 492), (555, 489), (560, 489), (565, 492), (560, 493), (555, 493)]\n",
      "The right_eye in this face has the following points: [(584, 489), (588, 486), (592, 486), (596, 487), (593, 489), (588, 490)]\n",
      "The top_lip in this face has the following points: [(566, 530), (572, 528), (578, 526), (581, 527), (584, 525), (587, 526), (590, 527), (588, 528), (584, 528), (581, 529), (578, 529), (568, 530)]\n",
      "The bottom_lip in this face has the following points: [(590, 527), (588, 531), (586, 534), (582, 535), (578, 535), (572, 534), (566, 530), (568, 530), (578, 530), (582, 530), (585, 528), (588, 528)]\n",
      "The chin in this face has the following points: [(878, 145), (879, 158), (880, 171), (883, 184), (888, 195), (895, 206), (903, 214), (914, 221), (926, 223), (937, 221), (948, 214), (957, 206), (964, 195), (968, 183), (970, 170), (972, 158), (972, 144)]\n",
      "The left_eyebrow in this face has the following points: [(886, 141), (892, 136), (902, 136), (911, 138), (919, 142)]\n",
      "The right_eyebrow in this face has the following points: [(932, 141), (940, 138), (949, 135), (958, 135), (965, 139)]\n",
      "The nose_bridge in this face has the following points: [(925, 149), (926, 159), (926, 168), (926, 178)]\n",
      "The nose_tip in this face has the following points: [(915, 180), (920, 182), (926, 184), (932, 182), (938, 179)]\n",
      "The left_eye in this face has the following points: [(895, 149), (901, 147), (907, 147), (913, 150), (907, 151), (901, 151)]\n",
      "The right_eye in this face has the following points: [(938, 150), (944, 146), (950, 146), (956, 148), (951, 150), (944, 151)]\n",
      "The top_lip in this face has the following points: [(907, 193), (914, 192), (921, 191), (926, 192), (931, 191), (938, 192), (945, 194), (942, 194), (931, 195), (926, 195), (921, 194), (910, 194)]\n",
      "The bottom_lip in this face has the following points: [(945, 194), (938, 200), (931, 202), (926, 203), (921, 202), (914, 199), (907, 193), (910, 194), (921, 196), (926, 197), (931, 197), (942, 194)]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import face_recognition\n",
    "# Load the jpg file into a numpy array\n",
    "image = face_recognition.load_image_file('/Users/shan/Desktop/1091627873644_.pic_hd.jpg' )\n",
    "\n",
    "# Find all facial features in all the faces in the image\n",
    "face_landmarks_list = face_recognition.face_landmarks(image)\n",
    "print('I found {} face(s) in this photograph.'.format(len(face_landmarks_list)))\n",
    "# Create a PIL imagedraw object so we can draw on the picture\n",
    "pil_image = Image.fromarray(image)\n",
    "d = ImageDraw.Draw(pil_image)\n",
    "i=1\n",
    "for face_landmarks in face_landmarks_list:\n",
    "    # Print the location of each facial feature in this image\n",
    "    for facial_feature in face_landmarks.keys():\n",
    "        print('The {} in this face has the following points: {}'.format(facial_feature, face_landmarks[facial_feature]))\n",
    "    # Letâ€™s trace out each facial feature in the image with a line!\n",
    "    for facial_feature in face_landmarks.keys():\n",
    "        d.line(face_landmarks[facial_feature], width=i)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "e39ff542",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "48e6e81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forever alone',\n",
       " 'forever alone',\n",
       " 'forever alone',\n",
       " 'forever alone',\n",
       " 'Match!',\n",
       " 'Match!',\n",
       " 'Match!',\n",
       " 'Match!']"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "38652748",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for i in range(0,len(face_landmarks_list)):\n",
    "    txt =results[i]\n",
    "    osd = Image.new(\"RGB\", (100,25), \"skyblue\")\n",
    "    dctx = ImageDraw.Draw(osd)  # create drawing context\n",
    "    dctx.text((5, 5), txt,  fill=\"black\") \n",
    "    x=face_landmarks_list[i]['left_eyebrow'] \n",
    "    (a,b)=[sum(y) / len(y) for y in zip(*x)]\n",
    "    a=int(a)\n",
    "    b=int(b)-50\n",
    "    pil_image.paste(\n",
    "    osd,\n",
    "    box=(a, b, osd.size[0] + a, osd.size[1] + b),\n",
    "    mask=Image.new(\"L\", osd.size, 192))\n",
    "    i+=1\n",
    "pil_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9708474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_landmarks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3196ee1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "9920369b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0,len(face_landmarks_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "700e145d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chin': [(878, 145),\n",
       "  (879, 158),\n",
       "  (880, 171),\n",
       "  (883, 184),\n",
       "  (888, 195),\n",
       "  (895, 206),\n",
       "  (903, 214),\n",
       "  (914, 221),\n",
       "  (926, 223),\n",
       "  (937, 221),\n",
       "  (948, 214),\n",
       "  (957, 206),\n",
       "  (964, 195),\n",
       "  (968, 183),\n",
       "  (970, 170),\n",
       "  (972, 158),\n",
       "  (972, 144)],\n",
       " 'left_eyebrow': [(886, 141), (892, 136), (902, 136), (911, 138), (919, 142)],\n",
       " 'right_eyebrow': [(932, 141), (940, 138), (949, 135), (958, 135), (965, 139)],\n",
       " 'nose_bridge': [(925, 149), (926, 159), (926, 168), (926, 178)],\n",
       " 'nose_tip': [(915, 180), (920, 182), (926, 184), (932, 182), (938, 179)],\n",
       " 'left_eye': [(895, 149),\n",
       "  (901, 147),\n",
       "  (907, 147),\n",
       "  (913, 150),\n",
       "  (907, 151),\n",
       "  (901, 151)],\n",
       " 'right_eye': [(938, 150),\n",
       "  (944, 146),\n",
       "  (950, 146),\n",
       "  (956, 148),\n",
       "  (951, 150),\n",
       "  (944, 151)],\n",
       " 'top_lip': [(907, 193),\n",
       "  (914, 192),\n",
       "  (921, 191),\n",
       "  (926, 192),\n",
       "  (931, 191),\n",
       "  (938, 192),\n",
       "  (945, 194),\n",
       "  (942, 194),\n",
       "  (931, 195),\n",
       "  (926, 195),\n",
       "  (921, 194),\n",
       "  (910, 194)],\n",
       " 'bottom_lip': [(945, 194),\n",
       "  (938, 200),\n",
       "  (931, 202),\n",
       "  (926, 203),\n",
       "  (921, 202),\n",
       "  (914, 199),\n",
       "  (907, 193),\n",
       "  (910, 194),\n",
       "  (921, 196),\n",
       "  (926, 197),\n",
       "  (931, 197),\n",
       "  (942, 194)]}"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_landmarks_list[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "12fd2d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pil_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "def6fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1=[]\n",
    "c2=[]\n",
    "c3=[]\n",
    "for i in range(1, 42):\n",
    "    if(i%3==0):\n",
    "        c1.append(i-2)\n",
    "        c2.append(i-1)\n",
    "        c3.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "239315cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "e7b3d9d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5, 8, 11, 14, 17, 20, 23, 26, 29, 32, 35, 38]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "a3cb449f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36, 39]"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "c27d1c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "options=np.zeros(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "a613e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "options[3]=1\n",
    "options[32]=1\n",
    "options[6]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "62bf9678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "95dfb28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "like_encode=[]\n",
    "for option in options:\n",
    "    if option:\n",
    "        like_encode.append(face_recognition.face_encodings(face_recognition.load_image_file(f\"/Users/shan/Desktop/data/p{i}.jpg\"))[0])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "7d80ba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = sum(like_encode)/len(like_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18d3d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1= face_recognition.load_image_file(\"/Users/shan/Desktop/data/p1.jpg\")\n",
    "image2= face_recognition.load_image_file(\"../raw_data/2.jpg\")\n",
    "image3= face_recognition.load_image_file(\"../raw_data/3.jpg\")\n",
    "#image4= face_recognition.load_image_file(\"../raw_data/4.jpg\")\n",
    "#image5= face_recognition.load_image_file(\"../raw_data/5.jpg\")\n",
    "image6= face_recognition.load_image_file(\"../raw_data/6.jpg\")\n",
    "image7= face_recognition.load_image_file(\"../raw_data/7.jpg\")\n",
    "image8= face_recognition.load_image_file(\"../raw_data/8.jpg\")\n",
    "encoding1 = face_recognition.face_encodings(image1)[0]\n",
    "encoding2 = face_recognition.face_encodings(image2)[0]\n",
    "encoding3 = face_recognition.face_encodings(image3)[0]\n",
    "#encoding4 = face_recognition.face_encodings(image4)[0]\n",
    "#encoding5 = face_recognition.face_encodings(image5)[0]\n",
    "encoding6 = face_recognition.face_encodings(image6)[0]\n",
    "encoding7 = face_recognition.face_encodings(image7)[0]\n",
    "encoding8 = face_recognition.face_encodings(image8)[0]\n",
    "avg = (encoding1+encoding2+encoding3+encoding6+encoding7+encoding8) / 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "dd276d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequential model.\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "#Getting necessary layers.\n",
    "from keras.layers import Conv2D             #Two-dimensional convolution layer.\n",
    "from keras.layers import MaxPooling2D       #Two-dimensional pooling layer.\n",
    "from keras.layers import Flatten            #Flattening layer.\n",
    "from keras.layers import Dropout            #Regularization to prevent overfitting.\n",
    "#Image preprocessing.\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "#Operating system interface.\n",
    "import os\n",
    "#Plotting library.\n",
    "import matplotlib.pyplot as plt\n",
    "#Other libraries.\n",
    "import numpy as np\n",
    "import random\n",
    "import PIL\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "#CNN Model\n",
    "def initialize_model():\n",
    "    model = Sequential()\n",
    "    #model.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "    #model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    #model.add(Flatten())\n",
    "    model.add(Dense(units = 128, input_dim = 128, activation = 'relu'))\n",
    "    model.add(Dense(units = 64, activation = 'relu'))\n",
    "    model.add(Dense(units = 100, activation = 'sigmoid'))\n",
    "    model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model\n",
    "model = initialize_model()\n",
    "def compile_model(model):\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "4bdc4505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               6500      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 31,369\n",
      "Trainable params: 31,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "7f812dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41,)"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "9b66bbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "like_encode=[]\n",
    "for option in options:\n",
    "    like_encode.append(face_recognition.face_encodings(face_recognition.load_image_file(f\"/Users/shan/Desktop/data/p{i+1}.jpg\"))[0])\n",
    "    i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "cf0364d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y=np.stack(like_encode, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "6e1dcf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_y=np.stack(options, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d69ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_y = pd.DataFrame(X_y)\n",
    "y_y =pd.DataFrame(y_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "cd32bfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3327 - accuracy: 0.9259\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.3113 - accuracy: 0.9259\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.2940 - accuracy: 0.9259\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2807 - accuracy: 0.9259\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2711 - accuracy: 0.9259\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2647 - accuracy: 0.9259\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.2610 - accuracy: 0.9259\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2594 - accuracy: 0.9259\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.2594 - accuracy: 0.9259\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.2605 - accuracy: 0.9259\n",
      "[0.2615847587585449, 0.9285714030265808]\n"
     ]
    }
   ],
   "source": [
    "#Yop's data\n",
    "# yop_data = pd.read_csv('./df.csv')\n",
    "X_train_y, X_test_y, y_train_y, y_test_y = train_test_split(X_y, y_y, test_size=0.33, random_state=42)\n",
    "model.fit(X_train_y, y_train_y, epochs=10, batch_size=32, verbose=1)\n",
    "print(model.evaluate(X_test_y, y_test_y, verbose=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "bca83993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.082185</td>\n",
       "      <td>0.087458</td>\n",
       "      <td>0.040287</td>\n",
       "      <td>0.009896</td>\n",
       "      <td>-0.034365</td>\n",
       "      <td>0.065845</td>\n",
       "      <td>0.027726</td>\n",
       "      <td>-0.020953</td>\n",
       "      <td>0.221836</td>\n",
       "      <td>0.033854</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033826</td>\n",
       "      <td>0.037516</td>\n",
       "      <td>0.083524</td>\n",
       "      <td>-0.039851</td>\n",
       "      <td>-0.172420</td>\n",
       "      <td>-0.061691</td>\n",
       "      <td>0.016066</td>\n",
       "      <td>-0.043019</td>\n",
       "      <td>-0.032126</td>\n",
       "      <td>0.043229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.128944</td>\n",
       "      <td>0.159561</td>\n",
       "      <td>0.055196</td>\n",
       "      <td>-0.107114</td>\n",
       "      <td>-0.099180</td>\n",
       "      <td>-0.049202</td>\n",
       "      <td>-0.001412</td>\n",
       "      <td>-0.124038</td>\n",
       "      <td>0.143565</td>\n",
       "      <td>0.094264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079931</td>\n",
       "      <td>0.054840</td>\n",
       "      <td>0.038349</td>\n",
       "      <td>-0.053082</td>\n",
       "      <td>-0.044711</td>\n",
       "      <td>-0.096816</td>\n",
       "      <td>0.123979</td>\n",
       "      <td>-0.015566</td>\n",
       "      <td>0.020104</td>\n",
       "      <td>0.042276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.122851</td>\n",
       "      <td>0.052817</td>\n",
       "      <td>0.035877</td>\n",
       "      <td>-0.087680</td>\n",
       "      <td>-0.092251</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>-0.021101</td>\n",
       "      <td>-0.053169</td>\n",
       "      <td>0.113835</td>\n",
       "      <td>-0.001913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028821</td>\n",
       "      <td>0.022526</td>\n",
       "      <td>0.042119</td>\n",
       "      <td>0.024457</td>\n",
       "      <td>-0.110612</td>\n",
       "      <td>-0.015928</td>\n",
       "      <td>0.111342</td>\n",
       "      <td>0.065457</td>\n",
       "      <td>-0.004186</td>\n",
       "      <td>0.019782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.059573</td>\n",
       "      <td>0.101811</td>\n",
       "      <td>0.041650</td>\n",
       "      <td>-0.110587</td>\n",
       "      <td>-0.080913</td>\n",
       "      <td>0.016195</td>\n",
       "      <td>-0.074347</td>\n",
       "      <td>-0.117296</td>\n",
       "      <td>0.174088</td>\n",
       "      <td>-0.099853</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090018</td>\n",
       "      <td>-0.025373</td>\n",
       "      <td>-0.029146</td>\n",
       "      <td>-0.061580</td>\n",
       "      <td>-0.160211</td>\n",
       "      <td>-0.131328</td>\n",
       "      <td>0.082560</td>\n",
       "      <td>-0.016304</td>\n",
       "      <td>0.072829</td>\n",
       "      <td>0.049280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.126217</td>\n",
       "      <td>0.131930</td>\n",
       "      <td>0.050875</td>\n",
       "      <td>-0.049826</td>\n",
       "      <td>-0.122907</td>\n",
       "      <td>0.028007</td>\n",
       "      <td>-0.006026</td>\n",
       "      <td>-0.071811</td>\n",
       "      <td>0.141167</td>\n",
       "      <td>-0.047902</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043068</td>\n",
       "      <td>0.020337</td>\n",
       "      <td>-0.083577</td>\n",
       "      <td>-0.080999</td>\n",
       "      <td>-0.043870</td>\n",
       "      <td>-0.052555</td>\n",
       "      <td>0.039418</td>\n",
       "      <td>-0.063028</td>\n",
       "      <td>0.099835</td>\n",
       "      <td>-0.023510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.082930</td>\n",
       "      <td>0.180904</td>\n",
       "      <td>0.002209</td>\n",
       "      <td>-0.057826</td>\n",
       "      <td>-0.100385</td>\n",
       "      <td>-0.029758</td>\n",
       "      <td>-0.082031</td>\n",
       "      <td>-0.096735</td>\n",
       "      <td>0.175512</td>\n",
       "      <td>-0.120450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158538</td>\n",
       "      <td>0.039858</td>\n",
       "      <td>0.076850</td>\n",
       "      <td>-0.070109</td>\n",
       "      <td>-0.073443</td>\n",
       "      <td>-0.082989</td>\n",
       "      <td>-0.008835</td>\n",
       "      <td>-0.007761</td>\n",
       "      <td>0.096595</td>\n",
       "      <td>-0.046744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.071433</td>\n",
       "      <td>0.056350</td>\n",
       "      <td>-0.023049</td>\n",
       "      <td>-0.098005</td>\n",
       "      <td>-0.088428</td>\n",
       "      <td>-0.082791</td>\n",
       "      <td>0.012730</td>\n",
       "      <td>-0.020926</td>\n",
       "      <td>0.198827</td>\n",
       "      <td>-0.008259</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018670</td>\n",
       "      <td>0.048942</td>\n",
       "      <td>0.061597</td>\n",
       "      <td>-0.106450</td>\n",
       "      <td>-0.162476</td>\n",
       "      <td>-0.055577</td>\n",
       "      <td>0.068136</td>\n",
       "      <td>-0.013442</td>\n",
       "      <td>-0.048085</td>\n",
       "      <td>-0.016204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.066139</td>\n",
       "      <td>0.139689</td>\n",
       "      <td>0.033647</td>\n",
       "      <td>-0.091200</td>\n",
       "      <td>-0.119572</td>\n",
       "      <td>-0.053987</td>\n",
       "      <td>-0.044789</td>\n",
       "      <td>-0.057445</td>\n",
       "      <td>0.096052</td>\n",
       "      <td>0.021658</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000306</td>\n",
       "      <td>-0.054255</td>\n",
       "      <td>0.054875</td>\n",
       "      <td>-0.072838</td>\n",
       "      <td>-0.080619</td>\n",
       "      <td>-0.060754</td>\n",
       "      <td>0.088317</td>\n",
       "      <td>-0.013936</td>\n",
       "      <td>0.043948</td>\n",
       "      <td>-0.007117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.156859</td>\n",
       "      <td>0.132606</td>\n",
       "      <td>-0.023587</td>\n",
       "      <td>0.037549</td>\n",
       "      <td>-0.084428</td>\n",
       "      <td>0.033747</td>\n",
       "      <td>-0.007457</td>\n",
       "      <td>-0.158476</td>\n",
       "      <td>0.238240</td>\n",
       "      <td>-0.076799</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117774</td>\n",
       "      <td>0.007040</td>\n",
       "      <td>0.051175</td>\n",
       "      <td>0.049823</td>\n",
       "      <td>-0.066527</td>\n",
       "      <td>-0.072149</td>\n",
       "      <td>0.087741</td>\n",
       "      <td>-0.057642</td>\n",
       "      <td>0.112168</td>\n",
       "      <td>-0.006209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.049821</td>\n",
       "      <td>0.079913</td>\n",
       "      <td>0.094981</td>\n",
       "      <td>-0.002513</td>\n",
       "      <td>-0.086902</td>\n",
       "      <td>-0.047164</td>\n",
       "      <td>-0.006087</td>\n",
       "      <td>-0.063675</td>\n",
       "      <td>0.098006</td>\n",
       "      <td>0.015423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000250</td>\n",
       "      <td>0.023101</td>\n",
       "      <td>0.010962</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>-0.047488</td>\n",
       "      <td>-0.057979</td>\n",
       "      <td>0.016428</td>\n",
       "      <td>-0.006247</td>\n",
       "      <td>-0.041743</td>\n",
       "      <td>-0.003299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.243099</td>\n",
       "      <td>0.169990</td>\n",
       "      <td>0.002372</td>\n",
       "      <td>-0.085929</td>\n",
       "      <td>-0.106806</td>\n",
       "      <td>0.017345</td>\n",
       "      <td>-0.074005</td>\n",
       "      <td>-0.109294</td>\n",
       "      <td>0.125068</td>\n",
       "      <td>-0.005989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187606</td>\n",
       "      <td>0.018016</td>\n",
       "      <td>0.056497</td>\n",
       "      <td>0.087680</td>\n",
       "      <td>-0.035012</td>\n",
       "      <td>-0.089749</td>\n",
       "      <td>0.097968</td>\n",
       "      <td>-0.061839</td>\n",
       "      <td>0.122663</td>\n",
       "      <td>0.063242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.091200</td>\n",
       "      <td>0.110727</td>\n",
       "      <td>0.110422</td>\n",
       "      <td>-0.114840</td>\n",
       "      <td>-0.140807</td>\n",
       "      <td>0.047672</td>\n",
       "      <td>-0.057034</td>\n",
       "      <td>-0.029785</td>\n",
       "      <td>0.192706</td>\n",
       "      <td>-0.143501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092912</td>\n",
       "      <td>0.015114</td>\n",
       "      <td>0.018531</td>\n",
       "      <td>-0.054735</td>\n",
       "      <td>-0.066772</td>\n",
       "      <td>-0.046864</td>\n",
       "      <td>0.122474</td>\n",
       "      <td>0.043640</td>\n",
       "      <td>0.075973</td>\n",
       "      <td>-0.049876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.184146</td>\n",
       "      <td>0.045107</td>\n",
       "      <td>0.108175</td>\n",
       "      <td>-0.028678</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>-0.063149</td>\n",
       "      <td>0.062278</td>\n",
       "      <td>-0.099393</td>\n",
       "      <td>0.196837</td>\n",
       "      <td>-0.055841</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006052</td>\n",
       "      <td>0.070098</td>\n",
       "      <td>-0.068432</td>\n",
       "      <td>-0.025023</td>\n",
       "      <td>-0.096970</td>\n",
       "      <td>0.020290</td>\n",
       "      <td>0.066154</td>\n",
       "      <td>0.043504</td>\n",
       "      <td>0.033036</td>\n",
       "      <td>0.018310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.138788</td>\n",
       "      <td>0.018757</td>\n",
       "      <td>0.126417</td>\n",
       "      <td>0.025617</td>\n",
       "      <td>-0.086873</td>\n",
       "      <td>-0.073704</td>\n",
       "      <td>0.056698</td>\n",
       "      <td>-0.128945</td>\n",
       "      <td>0.060911</td>\n",
       "      <td>-0.052701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021542</td>\n",
       "      <td>0.110182</td>\n",
       "      <td>-0.105705</td>\n",
       "      <td>-0.003189</td>\n",
       "      <td>-0.098972</td>\n",
       "      <td>-0.053650</td>\n",
       "      <td>0.149925</td>\n",
       "      <td>-0.006226</td>\n",
       "      <td>0.056416</td>\n",
       "      <td>0.023185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.156308</td>\n",
       "      <td>0.137480</td>\n",
       "      <td>0.082282</td>\n",
       "      <td>0.008709</td>\n",
       "      <td>-0.045542</td>\n",
       "      <td>-0.057501</td>\n",
       "      <td>0.030095</td>\n",
       "      <td>-0.142362</td>\n",
       "      <td>0.179592</td>\n",
       "      <td>-0.053588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054797</td>\n",
       "      <td>0.128517</td>\n",
       "      <td>-0.059002</td>\n",
       "      <td>0.003180</td>\n",
       "      <td>-0.087604</td>\n",
       "      <td>-0.004498</td>\n",
       "      <td>0.047659</td>\n",
       "      <td>-0.063468</td>\n",
       "      <td>0.151688</td>\n",
       "      <td>0.102815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.071730</td>\n",
       "      <td>0.055013</td>\n",
       "      <td>0.012633</td>\n",
       "      <td>-0.032538</td>\n",
       "      <td>-0.086116</td>\n",
       "      <td>-0.029756</td>\n",
       "      <td>0.034028</td>\n",
       "      <td>-0.139502</td>\n",
       "      <td>0.212234</td>\n",
       "      <td>-0.119048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035115</td>\n",
       "      <td>0.002315</td>\n",
       "      <td>0.106063</td>\n",
       "      <td>-0.139110</td>\n",
       "      <td>-0.171052</td>\n",
       "      <td>-0.060144</td>\n",
       "      <td>-0.050324</td>\n",
       "      <td>0.002910</td>\n",
       "      <td>0.029674</td>\n",
       "      <td>0.082499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.108481</td>\n",
       "      <td>0.106475</td>\n",
       "      <td>0.041305</td>\n",
       "      <td>-0.006812</td>\n",
       "      <td>-0.108451</td>\n",
       "      <td>-0.026292</td>\n",
       "      <td>-0.003983</td>\n",
       "      <td>-0.055617</td>\n",
       "      <td>0.198557</td>\n",
       "      <td>-0.008155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029361</td>\n",
       "      <td>0.087714</td>\n",
       "      <td>0.017959</td>\n",
       "      <td>-0.054930</td>\n",
       "      <td>-0.024069</td>\n",
       "      <td>-0.097847</td>\n",
       "      <td>-0.017171</td>\n",
       "      <td>-0.082829</td>\n",
       "      <td>0.066787</td>\n",
       "      <td>-0.032563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-0.142160</td>\n",
       "      <td>0.120042</td>\n",
       "      <td>-0.027557</td>\n",
       "      <td>0.032499</td>\n",
       "      <td>-0.025362</td>\n",
       "      <td>-0.083757</td>\n",
       "      <td>0.042180</td>\n",
       "      <td>-0.110825</td>\n",
       "      <td>0.148582</td>\n",
       "      <td>-0.030875</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004169</td>\n",
       "      <td>0.105037</td>\n",
       "      <td>-0.084791</td>\n",
       "      <td>-0.065481</td>\n",
       "      <td>-0.072878</td>\n",
       "      <td>-0.077422</td>\n",
       "      <td>-0.011503</td>\n",
       "      <td>-0.066957</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>-0.002205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.176809</td>\n",
       "      <td>0.113836</td>\n",
       "      <td>0.071473</td>\n",
       "      <td>-0.040820</td>\n",
       "      <td>-0.035315</td>\n",
       "      <td>0.009993</td>\n",
       "      <td>0.096559</td>\n",
       "      <td>-0.082416</td>\n",
       "      <td>0.315368</td>\n",
       "      <td>-0.081181</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000143</td>\n",
       "      <td>0.051924</td>\n",
       "      <td>0.030848</td>\n",
       "      <td>0.016479</td>\n",
       "      <td>-0.092681</td>\n",
       "      <td>-0.088208</td>\n",
       "      <td>-0.015093</td>\n",
       "      <td>-0.070840</td>\n",
       "      <td>-0.007597</td>\n",
       "      <td>0.031217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.167067</td>\n",
       "      <td>0.157974</td>\n",
       "      <td>0.088617</td>\n",
       "      <td>-0.015382</td>\n",
       "      <td>-0.003214</td>\n",
       "      <td>-0.068642</td>\n",
       "      <td>0.056617</td>\n",
       "      <td>-0.034997</td>\n",
       "      <td>0.131098</td>\n",
       "      <td>-0.059038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050394</td>\n",
       "      <td>0.071905</td>\n",
       "      <td>-0.045218</td>\n",
       "      <td>-0.068106</td>\n",
       "      <td>-0.090457</td>\n",
       "      <td>-0.106942</td>\n",
       "      <td>0.038903</td>\n",
       "      <td>-0.049169</td>\n",
       "      <td>-0.005835</td>\n",
       "      <td>0.029695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.170859</td>\n",
       "      <td>0.064411</td>\n",
       "      <td>0.118763</td>\n",
       "      <td>0.035973</td>\n",
       "      <td>-0.029907</td>\n",
       "      <td>-0.095951</td>\n",
       "      <td>0.002198</td>\n",
       "      <td>-0.129111</td>\n",
       "      <td>0.166811</td>\n",
       "      <td>0.019762</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018488</td>\n",
       "      <td>0.164744</td>\n",
       "      <td>-0.037007</td>\n",
       "      <td>0.017120</td>\n",
       "      <td>-0.048479</td>\n",
       "      <td>-0.044781</td>\n",
       "      <td>-0.058607</td>\n",
       "      <td>-0.063997</td>\n",
       "      <td>-0.039407</td>\n",
       "      <td>0.054361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-0.180545</td>\n",
       "      <td>0.113946</td>\n",
       "      <td>0.069412</td>\n",
       "      <td>-0.099944</td>\n",
       "      <td>-0.118911</td>\n",
       "      <td>-0.069482</td>\n",
       "      <td>-0.094760</td>\n",
       "      <td>-0.159823</td>\n",
       "      <td>0.132718</td>\n",
       "      <td>-0.112636</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119264</td>\n",
       "      <td>0.091873</td>\n",
       "      <td>-0.047817</td>\n",
       "      <td>0.036476</td>\n",
       "      <td>-0.178174</td>\n",
       "      <td>-0.001163</td>\n",
       "      <td>0.090692</td>\n",
       "      <td>-0.027452</td>\n",
       "      <td>0.085589</td>\n",
       "      <td>0.065328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.095056</td>\n",
       "      <td>0.037680</td>\n",
       "      <td>0.035157</td>\n",
       "      <td>-0.071495</td>\n",
       "      <td>-0.108276</td>\n",
       "      <td>-0.081187</td>\n",
       "      <td>-0.084548</td>\n",
       "      <td>-0.137376</td>\n",
       "      <td>0.146424</td>\n",
       "      <td>-0.079081</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111579</td>\n",
       "      <td>0.059065</td>\n",
       "      <td>0.032941</td>\n",
       "      <td>0.002898</td>\n",
       "      <td>-0.208923</td>\n",
       "      <td>-0.026162</td>\n",
       "      <td>0.091190</td>\n",
       "      <td>-0.078771</td>\n",
       "      <td>0.066370</td>\n",
       "      <td>0.030301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.081860</td>\n",
       "      <td>0.092856</td>\n",
       "      <td>0.039028</td>\n",
       "      <td>-0.083815</td>\n",
       "      <td>-0.129465</td>\n",
       "      <td>0.018109</td>\n",
       "      <td>-0.083454</td>\n",
       "      <td>-0.098433</td>\n",
       "      <td>0.144225</td>\n",
       "      <td>-0.160596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.167425</td>\n",
       "      <td>0.032398</td>\n",
       "      <td>0.012702</td>\n",
       "      <td>-0.041364</td>\n",
       "      <td>-0.230305</td>\n",
       "      <td>-0.009975</td>\n",
       "      <td>0.097113</td>\n",
       "      <td>-0.125071</td>\n",
       "      <td>0.090371</td>\n",
       "      <td>-0.006638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-0.083815</td>\n",
       "      <td>0.025330</td>\n",
       "      <td>0.015799</td>\n",
       "      <td>-0.070803</td>\n",
       "      <td>-0.074253</td>\n",
       "      <td>-0.072335</td>\n",
       "      <td>-0.060676</td>\n",
       "      <td>-0.163318</td>\n",
       "      <td>0.090127</td>\n",
       "      <td>-0.091361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109808</td>\n",
       "      <td>0.105372</td>\n",
       "      <td>-0.064422</td>\n",
       "      <td>-0.033064</td>\n",
       "      <td>-0.208633</td>\n",
       "      <td>-0.020266</td>\n",
       "      <td>0.073684</td>\n",
       "      <td>-0.053127</td>\n",
       "      <td>0.093611</td>\n",
       "      <td>0.027236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.076916</td>\n",
       "      <td>0.010459</td>\n",
       "      <td>0.105536</td>\n",
       "      <td>-0.104338</td>\n",
       "      <td>-0.118621</td>\n",
       "      <td>-0.069522</td>\n",
       "      <td>-0.071733</td>\n",
       "      <td>-0.131168</td>\n",
       "      <td>0.094646</td>\n",
       "      <td>-0.084260</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092614</td>\n",
       "      <td>0.086874</td>\n",
       "      <td>-0.034253</td>\n",
       "      <td>-0.000140</td>\n",
       "      <td>-0.148618</td>\n",
       "      <td>-0.001400</td>\n",
       "      <td>0.081638</td>\n",
       "      <td>-0.087969</td>\n",
       "      <td>0.133611</td>\n",
       "      <td>0.039454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.163617</td>\n",
       "      <td>0.081482</td>\n",
       "      <td>0.023760</td>\n",
       "      <td>-0.098140</td>\n",
       "      <td>-0.169816</td>\n",
       "      <td>-0.008826</td>\n",
       "      <td>-0.101568</td>\n",
       "      <td>-0.125044</td>\n",
       "      <td>0.150427</td>\n",
       "      <td>-0.139818</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064771</td>\n",
       "      <td>0.053248</td>\n",
       "      <td>-0.060094</td>\n",
       "      <td>-0.049600</td>\n",
       "      <td>-0.201678</td>\n",
       "      <td>0.006762</td>\n",
       "      <td>0.135350</td>\n",
       "      <td>-0.074534</td>\n",
       "      <td>0.135148</td>\n",
       "      <td>0.025377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-0.045672</td>\n",
       "      <td>0.074054</td>\n",
       "      <td>0.011734</td>\n",
       "      <td>-0.106872</td>\n",
       "      <td>-0.097606</td>\n",
       "      <td>-0.054966</td>\n",
       "      <td>-0.053371</td>\n",
       "      <td>-0.123803</td>\n",
       "      <td>0.165985</td>\n",
       "      <td>-0.134218</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111627</td>\n",
       "      <td>0.076203</td>\n",
       "      <td>0.043571</td>\n",
       "      <td>-0.037319</td>\n",
       "      <td>-0.262850</td>\n",
       "      <td>-0.001545</td>\n",
       "      <td>0.051057</td>\n",
       "      <td>-0.026379</td>\n",
       "      <td>0.130763</td>\n",
       "      <td>0.003596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.122695</td>\n",
       "      <td>0.141806</td>\n",
       "      <td>0.137949</td>\n",
       "      <td>-0.110309</td>\n",
       "      <td>-0.189630</td>\n",
       "      <td>-0.044277</td>\n",
       "      <td>-0.098797</td>\n",
       "      <td>-0.176135</td>\n",
       "      <td>0.174500</td>\n",
       "      <td>-0.153840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111324</td>\n",
       "      <td>0.050854</td>\n",
       "      <td>0.040574</td>\n",
       "      <td>0.020915</td>\n",
       "      <td>-0.214321</td>\n",
       "      <td>0.002485</td>\n",
       "      <td>0.017503</td>\n",
       "      <td>-0.145635</td>\n",
       "      <td>0.046086</td>\n",
       "      <td>0.015928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.021365</td>\n",
       "      <td>0.037510</td>\n",
       "      <td>0.051444</td>\n",
       "      <td>-0.055299</td>\n",
       "      <td>-0.082702</td>\n",
       "      <td>0.087511</td>\n",
       "      <td>-0.049919</td>\n",
       "      <td>-0.076754</td>\n",
       "      <td>0.172365</td>\n",
       "      <td>-0.086940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086630</td>\n",
       "      <td>0.111362</td>\n",
       "      <td>0.004449</td>\n",
       "      <td>0.051726</td>\n",
       "      <td>-0.191125</td>\n",
       "      <td>0.054239</td>\n",
       "      <td>0.072853</td>\n",
       "      <td>-0.072183</td>\n",
       "      <td>0.047721</td>\n",
       "      <td>-0.001650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-0.186039</td>\n",
       "      <td>0.179721</td>\n",
       "      <td>0.080325</td>\n",
       "      <td>-0.031291</td>\n",
       "      <td>-0.177405</td>\n",
       "      <td>0.012728</td>\n",
       "      <td>0.019689</td>\n",
       "      <td>-0.133091</td>\n",
       "      <td>0.216213</td>\n",
       "      <td>-0.094925</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086410</td>\n",
       "      <td>0.055782</td>\n",
       "      <td>0.039327</td>\n",
       "      <td>-0.022177</td>\n",
       "      <td>-0.148589</td>\n",
       "      <td>-0.023605</td>\n",
       "      <td>0.075423</td>\n",
       "      <td>-0.104873</td>\n",
       "      <td>0.094373</td>\n",
       "      <td>0.029020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-0.171464</td>\n",
       "      <td>0.117818</td>\n",
       "      <td>0.068933</td>\n",
       "      <td>-0.071028</td>\n",
       "      <td>-0.034423</td>\n",
       "      <td>0.008310</td>\n",
       "      <td>-0.038989</td>\n",
       "      <td>-0.064170</td>\n",
       "      <td>0.253294</td>\n",
       "      <td>-0.099237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037030</td>\n",
       "      <td>0.065520</td>\n",
       "      <td>-0.038709</td>\n",
       "      <td>-0.068614</td>\n",
       "      <td>-0.191484</td>\n",
       "      <td>-0.029442</td>\n",
       "      <td>0.005581</td>\n",
       "      <td>-0.087663</td>\n",
       "      <td>-0.018239</td>\n",
       "      <td>-0.013110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.074372</td>\n",
       "      <td>0.044262</td>\n",
       "      <td>0.028849</td>\n",
       "      <td>-0.066778</td>\n",
       "      <td>-0.036017</td>\n",
       "      <td>-0.033074</td>\n",
       "      <td>-0.028964</td>\n",
       "      <td>-0.102237</td>\n",
       "      <td>0.165528</td>\n",
       "      <td>-0.134203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064656</td>\n",
       "      <td>0.025035</td>\n",
       "      <td>0.038511</td>\n",
       "      <td>-0.073219</td>\n",
       "      <td>-0.177429</td>\n",
       "      <td>-0.096951</td>\n",
       "      <td>0.067299</td>\n",
       "      <td>-0.101620</td>\n",
       "      <td>0.084299</td>\n",
       "      <td>0.039731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>-0.093833</td>\n",
       "      <td>0.126549</td>\n",
       "      <td>0.168658</td>\n",
       "      <td>-0.126998</td>\n",
       "      <td>-0.108338</td>\n",
       "      <td>0.021801</td>\n",
       "      <td>-0.096786</td>\n",
       "      <td>-0.104542</td>\n",
       "      <td>0.156281</td>\n",
       "      <td>-0.094080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067311</td>\n",
       "      <td>0.074279</td>\n",
       "      <td>0.081328</td>\n",
       "      <td>-0.018591</td>\n",
       "      <td>-0.180768</td>\n",
       "      <td>-0.053022</td>\n",
       "      <td>0.023305</td>\n",
       "      <td>-0.047150</td>\n",
       "      <td>-0.066289</td>\n",
       "      <td>0.051274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.063075</td>\n",
       "      <td>0.013133</td>\n",
       "      <td>-0.005912</td>\n",
       "      <td>-0.092316</td>\n",
       "      <td>-0.074904</td>\n",
       "      <td>0.004718</td>\n",
       "      <td>-0.060630</td>\n",
       "      <td>-0.064308</td>\n",
       "      <td>0.140746</td>\n",
       "      <td>-0.095565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094864</td>\n",
       "      <td>0.109460</td>\n",
       "      <td>0.009861</td>\n",
       "      <td>-0.070462</td>\n",
       "      <td>-0.208993</td>\n",
       "      <td>-0.001172</td>\n",
       "      <td>0.040120</td>\n",
       "      <td>-0.113454</td>\n",
       "      <td>-0.031233</td>\n",
       "      <td>-0.032187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.098316</td>\n",
       "      <td>0.016941</td>\n",
       "      <td>0.045466</td>\n",
       "      <td>-0.110963</td>\n",
       "      <td>-0.106625</td>\n",
       "      <td>0.006283</td>\n",
       "      <td>-0.069934</td>\n",
       "      <td>-0.114634</td>\n",
       "      <td>0.193930</td>\n",
       "      <td>-0.172192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082955</td>\n",
       "      <td>0.038995</td>\n",
       "      <td>0.083733</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>-0.171010</td>\n",
       "      <td>0.021281</td>\n",
       "      <td>0.048992</td>\n",
       "      <td>-0.102428</td>\n",
       "      <td>-0.030999</td>\n",
       "      <td>-0.001161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.138828</td>\n",
       "      <td>0.022591</td>\n",
       "      <td>0.065673</td>\n",
       "      <td>-0.074224</td>\n",
       "      <td>-0.117963</td>\n",
       "      <td>-0.050584</td>\n",
       "      <td>-0.038823</td>\n",
       "      <td>-0.174657</td>\n",
       "      <td>0.060389</td>\n",
       "      <td>-0.017894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036386</td>\n",
       "      <td>0.068925</td>\n",
       "      <td>-0.006599</td>\n",
       "      <td>-0.096341</td>\n",
       "      <td>-0.063710</td>\n",
       "      <td>-0.016294</td>\n",
       "      <td>0.052836</td>\n",
       "      <td>-0.007798</td>\n",
       "      <td>-0.021078</td>\n",
       "      <td>0.051786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-0.135938</td>\n",
       "      <td>0.051216</td>\n",
       "      <td>0.007292</td>\n",
       "      <td>-0.088687</td>\n",
       "      <td>-0.015935</td>\n",
       "      <td>-0.022325</td>\n",
       "      <td>-0.066123</td>\n",
       "      <td>-0.166419</td>\n",
       "      <td>0.185593</td>\n",
       "      <td>-0.084239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046675</td>\n",
       "      <td>0.087927</td>\n",
       "      <td>0.011061</td>\n",
       "      <td>0.073725</td>\n",
       "      <td>-0.162145</td>\n",
       "      <td>-0.008072</td>\n",
       "      <td>0.061038</td>\n",
       "      <td>-0.071239</td>\n",
       "      <td>-0.001956</td>\n",
       "      <td>0.071759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.193095</td>\n",
       "      <td>0.115473</td>\n",
       "      <td>0.105208</td>\n",
       "      <td>-0.036899</td>\n",
       "      <td>-0.156228</td>\n",
       "      <td>-0.051569</td>\n",
       "      <td>-0.045158</td>\n",
       "      <td>-0.171896</td>\n",
       "      <td>0.138235</td>\n",
       "      <td>-0.110130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112841</td>\n",
       "      <td>0.032444</td>\n",
       "      <td>0.020523</td>\n",
       "      <td>-0.097851</td>\n",
       "      <td>-0.064879</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.089394</td>\n",
       "      <td>-0.073050</td>\n",
       "      <td>0.092765</td>\n",
       "      <td>-0.026101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.133912</td>\n",
       "      <td>0.130519</td>\n",
       "      <td>0.106091</td>\n",
       "      <td>-0.071230</td>\n",
       "      <td>-0.132094</td>\n",
       "      <td>0.016034</td>\n",
       "      <td>-0.063447</td>\n",
       "      <td>-0.097596</td>\n",
       "      <td>0.156941</td>\n",
       "      <td>-0.104257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.135836</td>\n",
       "      <td>0.075465</td>\n",
       "      <td>-0.008524</td>\n",
       "      <td>-0.066594</td>\n",
       "      <td>-0.101638</td>\n",
       "      <td>-0.057930</td>\n",
       "      <td>0.090092</td>\n",
       "      <td>-0.183490</td>\n",
       "      <td>0.072533</td>\n",
       "      <td>0.045379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.018395</td>\n",
       "      <td>0.122517</td>\n",
       "      <td>-0.001610</td>\n",
       "      <td>-0.015507</td>\n",
       "      <td>-0.137268</td>\n",
       "      <td>-0.046592</td>\n",
       "      <td>0.011731</td>\n",
       "      <td>-0.165748</td>\n",
       "      <td>0.250392</td>\n",
       "      <td>0.046451</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029009</td>\n",
       "      <td>0.139357</td>\n",
       "      <td>0.099128</td>\n",
       "      <td>-0.084402</td>\n",
       "      <td>-0.107026</td>\n",
       "      <td>0.010732</td>\n",
       "      <td>0.048819</td>\n",
       "      <td>-0.058389</td>\n",
       "      <td>0.046174</td>\n",
       "      <td>0.009105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41 rows Ã— 128 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6    \\\n",
       "0  -0.082185  0.087458  0.040287  0.009896 -0.034365  0.065845  0.027726   \n",
       "1  -0.128944  0.159561  0.055196 -0.107114 -0.099180 -0.049202 -0.001412   \n",
       "2  -0.122851  0.052817  0.035877 -0.087680 -0.092251  0.008600 -0.021101   \n",
       "3  -0.059573  0.101811  0.041650 -0.110587 -0.080913  0.016195 -0.074347   \n",
       "4  -0.126217  0.131930  0.050875 -0.049826 -0.122907  0.028007 -0.006026   \n",
       "5  -0.082930  0.180904  0.002209 -0.057826 -0.100385 -0.029758 -0.082031   \n",
       "6  -0.071433  0.056350 -0.023049 -0.098005 -0.088428 -0.082791  0.012730   \n",
       "7  -0.066139  0.139689  0.033647 -0.091200 -0.119572 -0.053987 -0.044789   \n",
       "8  -0.156859  0.132606 -0.023587  0.037549 -0.084428  0.033747 -0.007457   \n",
       "9  -0.049821  0.079913  0.094981 -0.002513 -0.086902 -0.047164 -0.006087   \n",
       "10 -0.243099  0.169990  0.002372 -0.085929 -0.106806  0.017345 -0.074005   \n",
       "11 -0.091200  0.110727  0.110422 -0.114840 -0.140807  0.047672 -0.057034   \n",
       "12 -0.184146  0.045107  0.108175 -0.028678  0.007072 -0.063149  0.062278   \n",
       "13 -0.138788  0.018757  0.126417  0.025617 -0.086873 -0.073704  0.056698   \n",
       "14 -0.156308  0.137480  0.082282  0.008709 -0.045542 -0.057501  0.030095   \n",
       "15 -0.071730  0.055013  0.012633 -0.032538 -0.086116 -0.029756  0.034028   \n",
       "16 -0.108481  0.106475  0.041305 -0.006812 -0.108451 -0.026292 -0.003983   \n",
       "17 -0.142160  0.120042 -0.027557  0.032499 -0.025362 -0.083757  0.042180   \n",
       "18 -0.176809  0.113836  0.071473 -0.040820 -0.035315  0.009993  0.096559   \n",
       "19 -0.167067  0.157974  0.088617 -0.015382 -0.003214 -0.068642  0.056617   \n",
       "20 -0.170859  0.064411  0.118763  0.035973 -0.029907 -0.095951  0.002198   \n",
       "21 -0.180545  0.113946  0.069412 -0.099944 -0.118911 -0.069482 -0.094760   \n",
       "22 -0.095056  0.037680  0.035157 -0.071495 -0.108276 -0.081187 -0.084548   \n",
       "23 -0.081860  0.092856  0.039028 -0.083815 -0.129465  0.018109 -0.083454   \n",
       "24 -0.083815  0.025330  0.015799 -0.070803 -0.074253 -0.072335 -0.060676   \n",
       "25 -0.076916  0.010459  0.105536 -0.104338 -0.118621 -0.069522 -0.071733   \n",
       "26 -0.163617  0.081482  0.023760 -0.098140 -0.169816 -0.008826 -0.101568   \n",
       "27 -0.045672  0.074054  0.011734 -0.106872 -0.097606 -0.054966 -0.053371   \n",
       "28 -0.122695  0.141806  0.137949 -0.110309 -0.189630 -0.044277 -0.098797   \n",
       "29 -0.021365  0.037510  0.051444 -0.055299 -0.082702  0.087511 -0.049919   \n",
       "30 -0.186039  0.179721  0.080325 -0.031291 -0.177405  0.012728  0.019689   \n",
       "31 -0.171464  0.117818  0.068933 -0.071028 -0.034423  0.008310 -0.038989   \n",
       "32 -0.074372  0.044262  0.028849 -0.066778 -0.036017 -0.033074 -0.028964   \n",
       "33 -0.093833  0.126549  0.168658 -0.126998 -0.108338  0.021801 -0.096786   \n",
       "34 -0.063075  0.013133 -0.005912 -0.092316 -0.074904  0.004718 -0.060630   \n",
       "35 -0.098316  0.016941  0.045466 -0.110963 -0.106625  0.006283 -0.069934   \n",
       "36 -0.138828  0.022591  0.065673 -0.074224 -0.117963 -0.050584 -0.038823   \n",
       "37 -0.135938  0.051216  0.007292 -0.088687 -0.015935 -0.022325 -0.066123   \n",
       "38 -0.193095  0.115473  0.105208 -0.036899 -0.156228 -0.051569 -0.045158   \n",
       "39 -0.133912  0.130519  0.106091 -0.071230 -0.132094  0.016034 -0.063447   \n",
       "40  0.018395  0.122517 -0.001610 -0.015507 -0.137268 -0.046592  0.011731   \n",
       "\n",
       "         7         8         9    ...       118       119       120       121  \\\n",
       "0  -0.020953  0.221836  0.033854  ... -0.033826  0.037516  0.083524 -0.039851   \n",
       "1  -0.124038  0.143565  0.094264  ...  0.079931  0.054840  0.038349 -0.053082   \n",
       "2  -0.053169  0.113835 -0.001913  ...  0.028821  0.022526  0.042119  0.024457   \n",
       "3  -0.117296  0.174088 -0.099853  ...  0.090018 -0.025373 -0.029146 -0.061580   \n",
       "4  -0.071811  0.141167 -0.047902  ...  0.043068  0.020337 -0.083577 -0.080999   \n",
       "5  -0.096735  0.175512 -0.120450  ...  0.158538  0.039858  0.076850 -0.070109   \n",
       "6  -0.020926  0.198827 -0.008259  ...  0.018670  0.048942  0.061597 -0.106450   \n",
       "7  -0.057445  0.096052  0.021658  ... -0.000306 -0.054255  0.054875 -0.072838   \n",
       "8  -0.158476  0.238240 -0.076799  ...  0.117774  0.007040  0.051175  0.049823   \n",
       "9  -0.063675  0.098006  0.015423  ...  0.000250  0.023101  0.010962  0.004500   \n",
       "10 -0.109294  0.125068 -0.005989  ...  0.187606  0.018016  0.056497  0.087680   \n",
       "11 -0.029785  0.192706 -0.143501  ...  0.092912  0.015114  0.018531 -0.054735   \n",
       "12 -0.099393  0.196837 -0.055841  ... -0.006052  0.070098 -0.068432 -0.025023   \n",
       "13 -0.128945  0.060911 -0.052701  ...  0.021542  0.110182 -0.105705 -0.003189   \n",
       "14 -0.142362  0.179592 -0.053588  ...  0.054797  0.128517 -0.059002  0.003180   \n",
       "15 -0.139502  0.212234 -0.119048  ...  0.035115  0.002315  0.106063 -0.139110   \n",
       "16 -0.055617  0.198557 -0.008155  ...  0.029361  0.087714  0.017959 -0.054930   \n",
       "17 -0.110825  0.148582 -0.030875  ... -0.004169  0.105037 -0.084791 -0.065481   \n",
       "18 -0.082416  0.315368 -0.081181  ... -0.000143  0.051924  0.030848  0.016479   \n",
       "19 -0.034997  0.131098 -0.059038  ...  0.050394  0.071905 -0.045218 -0.068106   \n",
       "20 -0.129111  0.166811  0.019762  ... -0.018488  0.164744 -0.037007  0.017120   \n",
       "21 -0.159823  0.132718 -0.112636  ...  0.119264  0.091873 -0.047817  0.036476   \n",
       "22 -0.137376  0.146424 -0.079081  ...  0.111579  0.059065  0.032941  0.002898   \n",
       "23 -0.098433  0.144225 -0.160596  ...  0.167425  0.032398  0.012702 -0.041364   \n",
       "24 -0.163318  0.090127 -0.091361  ...  0.109808  0.105372 -0.064422 -0.033064   \n",
       "25 -0.131168  0.094646 -0.084260  ...  0.092614  0.086874 -0.034253 -0.000140   \n",
       "26 -0.125044  0.150427 -0.139818  ...  0.064771  0.053248 -0.060094 -0.049600   \n",
       "27 -0.123803  0.165985 -0.134218  ...  0.111627  0.076203  0.043571 -0.037319   \n",
       "28 -0.176135  0.174500 -0.153840  ...  0.111324  0.050854  0.040574  0.020915   \n",
       "29 -0.076754  0.172365 -0.086940  ...  0.086630  0.111362  0.004449  0.051726   \n",
       "30 -0.133091  0.216213 -0.094925  ...  0.086410  0.055782  0.039327 -0.022177   \n",
       "31 -0.064170  0.253294 -0.099237  ...  0.037030  0.065520 -0.038709 -0.068614   \n",
       "32 -0.102237  0.165528 -0.134203  ...  0.064656  0.025035  0.038511 -0.073219   \n",
       "33 -0.104542  0.156281 -0.094080  ...  0.067311  0.074279  0.081328 -0.018591   \n",
       "34 -0.064308  0.140746 -0.095565  ...  0.094864  0.109460  0.009861 -0.070462   \n",
       "35 -0.114634  0.193930 -0.172192  ...  0.082955  0.038995  0.083733  0.002336   \n",
       "36 -0.174657  0.060389 -0.017894  ...  0.036386  0.068925 -0.006599 -0.096341   \n",
       "37 -0.166419  0.185593 -0.084239  ...  0.046675  0.087927  0.011061  0.073725   \n",
       "38 -0.171896  0.138235 -0.110130  ...  0.112841  0.032444  0.020523 -0.097851   \n",
       "39 -0.097596  0.156941 -0.104257  ...  0.135836  0.075465 -0.008524 -0.066594   \n",
       "40 -0.165748  0.250392  0.046451  ... -0.029009  0.139357  0.099128 -0.084402   \n",
       "\n",
       "         122       123       124       125       126       127  \n",
       "0  -0.172420 -0.061691  0.016066 -0.043019 -0.032126  0.043229  \n",
       "1  -0.044711 -0.096816  0.123979 -0.015566  0.020104  0.042276  \n",
       "2  -0.110612 -0.015928  0.111342  0.065457 -0.004186  0.019782  \n",
       "3  -0.160211 -0.131328  0.082560 -0.016304  0.072829  0.049280  \n",
       "4  -0.043870 -0.052555  0.039418 -0.063028  0.099835 -0.023510  \n",
       "5  -0.073443 -0.082989 -0.008835 -0.007761  0.096595 -0.046744  \n",
       "6  -0.162476 -0.055577  0.068136 -0.013442 -0.048085 -0.016204  \n",
       "7  -0.080619 -0.060754  0.088317 -0.013936  0.043948 -0.007117  \n",
       "8  -0.066527 -0.072149  0.087741 -0.057642  0.112168 -0.006209  \n",
       "9  -0.047488 -0.057979  0.016428 -0.006247 -0.041743 -0.003299  \n",
       "10 -0.035012 -0.089749  0.097968 -0.061839  0.122663  0.063242  \n",
       "11 -0.066772 -0.046864  0.122474  0.043640  0.075973 -0.049876  \n",
       "12 -0.096970  0.020290  0.066154  0.043504  0.033036  0.018310  \n",
       "13 -0.098972 -0.053650  0.149925 -0.006226  0.056416  0.023185  \n",
       "14 -0.087604 -0.004498  0.047659 -0.063468  0.151688  0.102815  \n",
       "15 -0.171052 -0.060144 -0.050324  0.002910  0.029674  0.082499  \n",
       "16 -0.024069 -0.097847 -0.017171 -0.082829  0.066787 -0.032563  \n",
       "17 -0.072878 -0.077422 -0.011503 -0.066957  0.002304 -0.002205  \n",
       "18 -0.092681 -0.088208 -0.015093 -0.070840 -0.007597  0.031217  \n",
       "19 -0.090457 -0.106942  0.038903 -0.049169 -0.005835  0.029695  \n",
       "20 -0.048479 -0.044781 -0.058607 -0.063997 -0.039407  0.054361  \n",
       "21 -0.178174 -0.001163  0.090692 -0.027452  0.085589  0.065328  \n",
       "22 -0.208923 -0.026162  0.091190 -0.078771  0.066370  0.030301  \n",
       "23 -0.230305 -0.009975  0.097113 -0.125071  0.090371 -0.006638  \n",
       "24 -0.208633 -0.020266  0.073684 -0.053127  0.093611  0.027236  \n",
       "25 -0.148618 -0.001400  0.081638 -0.087969  0.133611  0.039454  \n",
       "26 -0.201678  0.006762  0.135350 -0.074534  0.135148  0.025377  \n",
       "27 -0.262850 -0.001545  0.051057 -0.026379  0.130763  0.003596  \n",
       "28 -0.214321  0.002485  0.017503 -0.145635  0.046086  0.015928  \n",
       "29 -0.191125  0.054239  0.072853 -0.072183  0.047721 -0.001650  \n",
       "30 -0.148589 -0.023605  0.075423 -0.104873  0.094373  0.029020  \n",
       "31 -0.191484 -0.029442  0.005581 -0.087663 -0.018239 -0.013110  \n",
       "32 -0.177429 -0.096951  0.067299 -0.101620  0.084299  0.039731  \n",
       "33 -0.180768 -0.053022  0.023305 -0.047150 -0.066289  0.051274  \n",
       "34 -0.208993 -0.001172  0.040120 -0.113454 -0.031233 -0.032187  \n",
       "35 -0.171010  0.021281  0.048992 -0.102428 -0.030999 -0.001161  \n",
       "36 -0.063710 -0.016294  0.052836 -0.007798 -0.021078  0.051786  \n",
       "37 -0.162145 -0.008072  0.061038 -0.071239 -0.001956  0.071759  \n",
       "38 -0.064879  0.001399  0.089394 -0.073050  0.092765 -0.026101  \n",
       "39 -0.101638 -0.057930  0.090092 -0.183490  0.072533  0.045379  \n",
       "40 -0.107026  0.010732  0.048819 -0.058389  0.046174  0.009105  \n",
       "\n",
       "[41 rows x 128 columns]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e0a7a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "56dae963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1=(X_y[1]+X_y[2])/2\n",
    "test1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "17bcd31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=np.stack(test1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "12257289",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /Users/shan/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/keras/engine/training.py:1544 predict_function  *\n        return step_function(self, iterator)\n    /Users/shan/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/keras/engine/training.py:1527 run_step  *\n        outputs = model.predict_step(data)\n    /Users/shan/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/keras/engine/training.py:1500 predict_step  *\n        return self(x, training=False)\n    /Users/shan/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/keras/engine/base_layer.py:989 __call__  *\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /Users/shan/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/keras/engine/input_spec.py:248 assert_input_compatibility  *\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_2 is incompatible with the layer: expected axis -1 of input shape to have value 128 but received input with shape (32, 1)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-349-910851cb7dff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1700\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1702\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m-> 3022\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3439\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[0;32m-> 3440\u001b[0;31m             return self._define_function_with_shape_relaxation(\n\u001b[0m\u001b[1;32m   3441\u001b[0m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3360\u001b[0m           expand_composites=True)\n\u001b[1;32m   3361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3362\u001b[0;31m     graph_function = self._create_graph_function(\n\u001b[0m\u001b[1;32m   3363\u001b[0m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[1;32m   3364\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3279\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /Users/shan/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/keras/engine/training.py:1544 predict_function  *\n        return step_function(self, iterator)\n    /Users/shan/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/keras/engine/training.py:1527 run_step  *\n        outputs = model.predict_step(data)\n    /Users/shan/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/keras/engine/training.py:1500 predict_step  *\n        return self(x, training=False)\n    /Users/shan/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/keras/engine/base_layer.py:989 __call__  *\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /Users/shan/.pyenv/versions/3.8.6/envs/lewagon/lib/python3.8/site-packages/keras/engine/input_spec.py:248 assert_input_compatibility  *\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_2 is incompatible with the layer: expected axis -1 of input shape to have value 128 but received input with shape (32, 1)\n"
     ]
    }
   ],
   "source": [
    "model.predict(test).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f90ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('/Users/shan/Desktop/lw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "ad11a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=test_set.iloc[: , 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "5a56d55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15,  8, 17, 14, 13,  4,  1, 18, 12,  9,  2, 16, 10,  7,  3,  6,  5,\n",
       "       11])"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = model.predict(test).T\n",
    "def pred_rank(x):\n",
    "    return np.argsort(x)[0] + 1\n",
    "pred_rank(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "d5fd0a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "names=['Zendaya', 'Justin Bieber', 'Taylor Rapp', 'Michael B Jordan', 'é»„è§‰', 'Timothee Chalamet', 'Penn Badgley', 'æŽå‡†åŸº', 'Gal Gadot', 'Chadwick Boseman', 'Iamdapower', 'Victoria Pedretti', 'Octavia Spencer', 'å½­äºŽæ™', 'é»„å­éŸ¬', 'Sarah Paulson', 'Lucy Liu', 'P Elepant', 'èˆ’æ·‡', 'ä¼Šè—¤ç¾Žè¯š', 'è¾£ç›®æ´‹å­', 'Jinmin', 'Matt Severson', 'Tylehner', 'é™ˆå¦å¸Œ', 'æ™¯ç”œ', 'Aaron Donald', 'ç¦åŽŸçˆ±', 'åˆ˜é›¯', 'Miguel Silvestre', 'Benedict Cumberbatch', 'Ariel Winter', 'Naomi Osaka', 'Awkwafina', 'Chris Hemsworth', 'Prince Harry', 'Alex Pall', 'Keanu Reeves', 'Russel Tovey', 'Beyonce', 'Ed Sheeran', 'Kat Dennings', 'Sarah Hyland', 'Nolan Gould', 'Leo Chang', 'Dan Raynolds', 'larray', 'è®¸å‡¯', 'James Corden', 'Adele', 'James Charles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "4f6f1af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mv: rename /Users/shan/Desktop/interface_face/Zendaya.jpg to /Users/chaitanyasingh/Documents/0.jpg: No such file or directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "mv: rename /Users/shan/Desktop/interface_face/é»„è§‰.jpg to /Users/chaitanyasingh/Documents/4.jpg: No such file or directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "mv: rename /Users/shan/Desktop/interface_face/æŽå‡†åŸº.jpg to /Users/chaitanyasingh/Documents/7.jpg: No such file or directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "mv: rename /Users/shan/Desktop/interface_face/Iamdapower.jpg to /Users/chaitanyasingh/Documents/10.jpg: No such file or directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "mv: rename /Users/shan/Desktop/interface_face/å½­äºŽæ™.jpg to /Users/chaitanyasingh/Documents/13.jpg: No such file or directory\n",
      "mv: rename /Users/shan/Desktop/interface_face/é»„å­éŸ¬.jpg to /Users/chaitanyasingh/Documents/14.jpg: No such file or directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "mv: rename /Users/shan/Desktop/interface_face/èˆ’æ·‡.jpg to /Users/chaitanyasingh/Documents/18.jpg: No such file or directory\n",
      "mv: rename /Users/shan/Desktop/interface_face/ä¼Šè—¤ç¾Žè¯š.jpg to /Users/chaitanyasingh/Documents/19.jpg: No such file or directory\n",
      "mv: rename /Users/shan/Desktop/interface_face/è¾£ç›®æ´‹å­.jpg to /Users/chaitanyasingh/Documents/20.jpg: No such file or directory\n",
      "mv: rename /Users/shan/Desktop/interface_face/Jinmin.jpg to /Users/chaitanyasingh/Documents/21.jpg: No such file or directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "mv: rename /Users/shan/Desktop/interface_face/Tylehner.jpg to /Users/chaitanyasingh/Documents/23.jpg: No such file or directory\n",
      "mv: rename /Users/shan/Desktop/interface_face/é™ˆå¦å¸Œ.jpg to /Users/chaitanyasingh/Documents/24.jpg: No such file or directory\n",
      "mv: rename /Users/shan/Desktop/interface_face/æ™¯ç”œ.jpg to /Users/chaitanyasingh/Documents/25.jpg: No such file or directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "mv: rename /Users/shan/Desktop/interface_face/ç¦åŽŸçˆ±.jpg to /Users/chaitanyasingh/Documents/27.jpg: No such file or directory\n",
      "mv: rename /Users/shan/Desktop/interface_face/åˆ˜é›¯.jpg to /Users/chaitanyasingh/Documents/28.jpg: No such file or directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "mv: rename /Users/shan/Desktop/interface_face/Awkwafina.jpg to /Users/chaitanyasingh/Documents/33.jpg: No such file or directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "mv: rename /Users/shan/Desktop/interface_face/Beyonce.jpg to /Users/chaitanyasingh/Documents/39.jpg: No such file or directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "mv: rename /Users/shan/Desktop/interface_face/larray.jpg to /Users/chaitanyasingh/Documents/46.jpg: No such file or directory\n",
      "mv: rename /Users/shan/Desktop/interface_face/è®¸å‡¯.jpg to /Users/chaitanyasingh/Documents/47.jpg: No such file or directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n",
      "mv: rename /Users/shan/Desktop/interface_face/Adele.jpg to /Users/chaitanyasingh/Documents/49.jpg: No such file or directory\n",
      "usage: mv [-f | -i | -n] [-v] source target\n",
      "       mv [-f | -i | -n] [-v] source ... directory\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for name in names:\n",
    "    !mv /Users/shan/Desktop/interface_face/{name}.jpg /Users/chaitanyasingh/Documents/{i}.jpg\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "20efb019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.15534931e-02,  1.76486820e-02,  6.17726594e-02, -2.40194164e-02,\n",
       "       -1.06809825e-01, -7.45332241e-03, -9.66395587e-02, -7.02777803e-02,\n",
       "        1.93047166e-01, -9.48556066e-02,  1.66920573e-01, -5.47702983e-02,\n",
       "       -1.76777750e-01, -7.64412433e-03, -4.20218930e-02,  1.86352283e-01,\n",
       "       -1.40480667e-01, -1.31473944e-01, -7.64039457e-02, -4.76862267e-02,\n",
       "        7.09119737e-02,  2.60485373e-02,  6.18044958e-02,  1.56119227e-01,\n",
       "       -1.65665597e-01, -3.46837312e-01,  1.66435838e-02, -1.12717152e-02,\n",
       "       -2.43016165e-02, -9.77950096e-02, -5.82340211e-02,  1.71735287e-01,\n",
       "       -2.40639582e-01,  1.04329064e-02, -4.60730642e-02,  1.77250445e-01,\n",
       "       -4.50186580e-02, -1.07856698e-01,  1.16953202e-01,  9.28352475e-02,\n",
       "       -2.29868084e-01, -3.37341279e-02, -4.07465808e-02,  2.11454421e-01,\n",
       "        2.36732632e-01,  3.75924632e-03,  7.92644396e-02, -6.48279786e-02,\n",
       "        1.90699175e-02, -3.06202054e-01, -1.54443178e-03,  1.78814232e-01,\n",
       "       -1.22470986e-02,  2.65415981e-02,  3.06749977e-02, -2.07913026e-01,\n",
       "       -2.37063766e-02,  7.52966702e-02, -9.13689211e-02, -1.35503104e-02,\n",
       "        7.11053610e-03, -4.93530408e-02,  4.07424867e-02, -1.41411349e-01,\n",
       "        2.51469940e-01,  1.46725059e-01, -8.81846547e-02, -9.85058844e-02,\n",
       "        1.44811615e-01, -1.53512672e-01, -9.12511721e-02,  3.43252271e-02,\n",
       "       -1.18683718e-01, -1.91121310e-01, -3.49752009e-01, -1.65312737e-02,\n",
       "        3.55331659e-01,  1.17996164e-01, -1.69191808e-01,  8.35983530e-02,\n",
       "       -4.41041626e-02,  7.12164491e-03,  8.00179690e-03,  1.79469019e-01,\n",
       "       -2.06864327e-02,  8.52837935e-02, -6.66221008e-02,  4.09394950e-02,\n",
       "        1.66765079e-01,  1.79904476e-02, -1.62791722e-02,  2.28920281e-01,\n",
       "        2.60894746e-03, -1.97603256e-02,  5.39542139e-02,  1.61156058e-02,\n",
       "       -7.50608370e-02,  4.85696271e-02, -1.21714070e-01,  7.17793405e-03,\n",
       "       -6.30611777e-02, -1.37244165e-02,  1.44014210e-02,  9.05333906e-02,\n",
       "       -1.51198387e-01,  1.45396829e-01, -5.15314117e-02, -3.31048369e-02,\n",
       "       -1.67153776e-04,  2.49976516e-02, -1.09292403e-01, -6.86233863e-02,\n",
       "        1.27576113e-01, -2.59830713e-01,  9.21223462e-02,  1.86675400e-01,\n",
       "        9.21400189e-02,  1.94727331e-01,  6.22399971e-02,  1.79737732e-01,\n",
       "       -1.01743937e-02,  5.48174977e-02, -1.44175723e-01,  1.56993009e-02,\n",
       "        2.79783905e-02, -5.81822470e-02, -9.27843153e-03,  9.38481763e-02])"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_recognition.face_encodings(face_recognition.load_image_file(f\"/Users/shan/Desktop/interface_face/51.jpg\"))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "8215bf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_faces={1: 20,\n",
    " 2: 20,\n",
    " 3: 20,\n",
    " 4: 19,\n",
    " 5: 20,\n",
    " 6: 20,\n",
    " 7: 20,\n",
    " 8: 19,\n",
    " 9: 20,\n",
    " 10: 20,\n",
    " 11: 19,\n",
    " 12: 19,\n",
    " 13: 20,\n",
    " 14: 20,\n",
    " 15: 20,\n",
    " 16: 19,\n",
    " 17: 19,\n",
    " 18: 20,\n",
    " 19: 18,\n",
    " 20: 20,\n",
    " 21: 20,\n",
    " 22: 20,\n",
    " 23: 20,\n",
    " 24: 20,\n",
    " 25: 20,\n",
    " 26: 20,\n",
    " 27: 20,\n",
    " 28: 19,\n",
    " 29: 19,\n",
    " 30: 20,\n",
    " 31: 17,\n",
    " 32: 20,\n",
    " 33: 20,\n",
    " 34: 19,\n",
    " 35: 20,\n",
    " 36: 20,\n",
    " 37: 20,\n",
    " 38: 20,\n",
    " 39: 19,\n",
    " 40: 18,\n",
    " 41: 20,\n",
    " 42: 19,\n",
    " 43: 20,\n",
    " 44: 20,\n",
    " 45: 20,\n",
    " 46: 20,\n",
    " 47: 18,\n",
    " 48: 9,\n",
    " 49: 19,\n",
    " 50: 20,\n",
    " 51: 19,\n",
    " 52: 19}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "1ba66130",
   "metadata": {},
   "outputs": [],
   "source": [
    "options=['0.0', '0.0','0.0','0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '1.0', '1.0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "f66b6fb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '0.0',\n",
       " '1.0',\n",
       " '1.0']"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "80194bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(options*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "1d8554f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = [0 if option=='0.0' else 1 for option in options]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "136122d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[]\n",
    "for i in range(1,53):\n",
    "    for item in num_faces[i]*[options[i-1]]:\n",
    "        y.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "4f97bb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "c0be3e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "0ca9eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "d4ea538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=pickle.load(open('/Users/shan/Desktop/cele50_encode', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "729a527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(patience=3)\n",
    "reg_l1 = regularizers.L1(0.01)\n",
    "reg_l2 = regularizers.L2(0.01)\n",
    "#CNN Model\n",
    "def initialize_model():\n",
    "    model = Sequential()\n",
    "    #model.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "    #model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "    #model.add(Flatten())\n",
    "    model.add(Dense(units = 128, input_dim = 128, activation = 'relu'))\n",
    "    model.add(Dense(units = 64, activation = 'relu', kernel_regularizer=reg_l1))\n",
    "    model.add(Dense(units = 100, activation = 'sigmoid', bias_regularizer=reg_l2))\n",
    "    model.add(Dense(units = 50, activation = 'sigmoid'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model\n",
    "model = initialize_model()\n",
    "def compile_model(model):\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "0db136f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.sequential.Sequential at 0x15989c850>"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "9198b9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "26/26 [==============================] - 3s 14ms/step - loss: 7.0414 - accuracy: 0.9618 - val_loss: 5.4268 - val_accuracy: 0.9752\n",
      "Epoch 2/30\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 5.0442 - accuracy: 0.9583 - val_loss: 3.7735 - val_accuracy: 0.9752\n",
      "Epoch 3/30\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 3.5132 - accuracy: 0.9452 - val_loss: 2.4288 - val_accuracy: 0.9752\n",
      "Epoch 4/30\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 2.2070 - accuracy: 0.9582 - val_loss: 1.3907 - val_accuracy: 0.9752\n",
      "Epoch 5/30\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 1.2560 - accuracy: 0.9561 - val_loss: 0.6584 - val_accuracy: 0.9752\n",
      "Epoch 6/30\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.5905 - accuracy: 0.9564 - val_loss: 0.2472 - val_accuracy: 0.9752\n",
      "Epoch 7/30\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.2700 - accuracy: 0.9559 - val_loss: 0.1446 - val_accuracy: 0.9752\n",
      "Epoch 8/30\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1687 - accuracy: 0.9674 - val_loss: 0.1310 - val_accuracy: 0.9752\n",
      "Epoch 9/30\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1688 - accuracy: 0.9632 - val_loss: 0.1323 - val_accuracy: 0.9752\n",
      "Epoch 10/30\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1958 - accuracy: 0.9537 - val_loss: 0.1298 - val_accuracy: 0.9752\n",
      "Epoch 11/30\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1749 - accuracy: 0.9618 - val_loss: 0.1285 - val_accuracy: 0.9752\n",
      "Epoch 12/30\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1584 - accuracy: 0.9662 - val_loss: 0.1289 - val_accuracy: 0.9752\n",
      "Epoch 13/30\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1665 - accuracy: 0.9642 - val_loss: 0.1285 - val_accuracy: 0.9752\n",
      "Epoch 14/30\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1773 - accuracy: 0.9590 - val_loss: 0.1300 - val_accuracy: 0.9752\n",
      "Epoch 15/30\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1654 - accuracy: 0.9646 - val_loss: 0.1303 - val_accuracy: 0.9752\n",
      "Epoch 16/30\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1920 - accuracy: 0.9570 - val_loss: 0.1319 - val_accuracy: 0.9752\n",
      "Epoch 17/30\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1879 - accuracy: 0.9576 - val_loss: 0.1287 - val_accuracy: 0.9752\n",
      "Epoch 18/30\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1791 - accuracy: 0.9609 - val_loss: 0.1291 - val_accuracy: 0.9752\n",
      "Epoch 19/30\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1941 - accuracy: 0.9557 - val_loss: 0.1298 - val_accuracy: 0.9752\n",
      "Epoch 20/30\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1899 - accuracy: 0.9578 - val_loss: 0.1291 - val_accuracy: 0.9752\n",
      "Epoch 21/30\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1535 - accuracy: 0.9685 - val_loss: 0.1295 - val_accuracy: 0.9752\n",
      "Epoch 22/30\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1772 - accuracy: 0.9607 - val_loss: 0.1301 - val_accuracy: 0.9752\n",
      "Epoch 23/30\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1988 - accuracy: 0.9531 - val_loss: 0.1310 - val_accuracy: 0.9752\n",
      "Epoch 24/30\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1842 - accuracy: 0.9581 - val_loss: 0.1289 - val_accuracy: 0.9752\n",
      "Epoch 25/30\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1893 - accuracy: 0.9564 - val_loss: 0.1282 - val_accuracy: 0.9752\n",
      "Epoch 26/30\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1525 - accuracy: 0.9686 - val_loss: 0.1289 - val_accuracy: 0.9752\n",
      "Epoch 27/30\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1874 - accuracy: 0.9576 - val_loss: 0.1303 - val_accuracy: 0.9752\n",
      "Epoch 28/30\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.2073 - accuracy: 0.9514 - val_loss: 0.1280 - val_accuracy: 0.9752\n",
      "Epoch 29/30\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.1860 - accuracy: 0.9582 - val_loss: 0.1288 - val_accuracy: 0.9752\n",
      "Epoch 30/30\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1798 - accuracy: 0.9599 - val_loss: 0.1287 - val_accuracy: 0.9752\n",
      "[0.28016331791877747, 0.9285714030265808]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=32, verbose=1, shuffle=True)\n",
    "print(model.evaluate(X_test_y, y_test_y, verbose=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e035e66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "test_set = pd.read_csv('./lw.csv')\n",
    "label = model.predict(test_set).T\n",
    "def pred_rank(x):\n",
    "    return np.argsort(x)[0] + 1\n",
    "print(pred_rank(label))\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "#history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "7734e732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03749207]], dtype=float32)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "8f641d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg=np.array([-0.05895312,  0.0745253 ,  0.03380865, -0.04731128, -0.08491885,\n",
    "       -0.02961966, -0.05191092, -0.10862959,  0.14372534, -0.07666109,\n",
    "        0.27531608, -0.00215914, -0.22524711, -0.10346345, -0.02881772,\n",
    "        0.10590863, -0.15179179, -0.10992581, -0.01464245, -0.03897626,\n",
    "        0.09400089,  0.03049925,  0.04361126,  0.02129004, -0.11924487,\n",
    "       -0.30149831, -0.06317274, -0.0645417 ,  0.06719439, -0.10945869,\n",
    "        0.0076423 ,  0.05229391, -0.19245138, -0.06429987,  0.02065386,\n",
    "        0.08652538, -0.07043852, -0.06225016,  0.19108021,  0.03459134,\n",
    "       -0.14663414,  0.01323044, -0.02724313,  0.27925713,  0.18702968,\n",
    "        0.03974114,  0.02003187, -0.15192135,  0.10985727, -0.24433079,\n",
    "        0.07107684,  0.16333052,  0.07042256,  0.06401812,  0.0436044 ,\n",
    "       -0.15644974,  0.04818465,  0.10099875, -0.19695896,  0.06098775,\n",
    "        0.06687803, -0.02233723,  0.00497208, -0.08505531,  0.21065185,\n",
    "        0.05436892, -0.11417659, -0.14432085,  0.11005539, -0.13054259,\n",
    "       -0.10125365,  0.11344694, -0.13645897, -0.19573336, -0.29942531,\n",
    "        0.02756927,  0.35478593,  0.12477077, -0.18834226,  0.00852417,\n",
    "       -0.05529355, -0.00863172,  0.10900245,  0.06525503, -0.03068945,\n",
    "       -0.02844287, -0.10301634,  0.00854152,  0.19495285, -0.03673798,\n",
    "       -0.09611909,  0.23435274, -0.00139264,  0.06649028,  0.05265563,\n",
    "        0.05891484, -0.07191548,  0.03963904, -0.13435482, -0.03143024,\n",
    "        0.04344594, -0.07645156, -0.00183903,  0.13006889, -0.15507843,\n",
    "        0.17883338, -0.01178621,  0.01827412,  0.01456133, -0.05132867,\n",
    "       -0.13738021, -0.00994921,  0.15100468, -0.28005914,  0.26580951,\n",
    "        0.18423581,  0.10626658,  0.12092329,  0.08839736,  0.04223279,\n",
    "       -0.02754693, -0.0313828 , -0.16761477, -0.04406595,  0.02360056,\n",
    "       -0.02829835,  0.05815861,  0.00882281])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "20aab538",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg=avg.reshape(1,128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d37ccdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
